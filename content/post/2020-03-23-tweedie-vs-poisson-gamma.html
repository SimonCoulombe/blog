---
title: Tweedie vs Poisson * Gamma
author: Simon
date: '2020-03-23'
slug: tweedie-vs-poisson-gamma
categories:
  - R
  - insurnace
tags: []
keywords:
  - tech
---



<p>I’m building my first tweedie model, and I’m finally trying the {recipes} package.</p>
<p>We will try to predict the pure premium of car insurance policy. This can be done directly with a tweedie model, or by multiplying two separates models: a frequency (Poisson) and a severity (Gamma) model. We wil be using “lift charts” and “double lift charts” to evaluate the model performance .</p>
<p>Here’s is the plan:</p>
<ul>
<li>Pre-process the train and test data using <code>recipes</code>.<br />
</li>
<li>Estimate the pure premium directly using a Tweedie model (93% of <code>claimcst0</code>values are 0$, the others are the claims dollar amount).<br />
</li>
<li>Estimate the frequency of claims using a Poisson model (93% of values <code>clm</code>values are 0, the other are equal to 1).</li>
<li>Estimate the severity of claims using a Gamma model (estimate the <code>claimcst0</code> value for the 7% that is above 0$)<br />
</li>
<li>All three models are estimated using a GLM and a GBM approach.<br />
</li>
<li>Models are compared using three approaches :
<ul>
<li>Normalized Gini Index,</li>
<li>Lift charts,<br />
</li>
<li>Double-lift charts.</li>
</ul></li>
</ul>
<p>More details <a href="https://www.casact.org/education/rpm/2015/handouts/Paper_4034_handout_2419_0.pdf">about these charts here</a>. What I call “lift chart” is named “simple quantile plots” in the pdf.</p>
<p>tl;dr: <code>recipes</code> is AWESOME. Also, both glm and xgboost models all perform poorly. <a href="https://www.simoncoulombe.com/2020/03/tweedue-and-recipes-part-2-kaggle-data/">I made a new post using 6M observatiosn instead of 67k, and it helps.</a></p>
<p>Reasons I will come back to this post for code snippets:<br />
* the rsample, yardstick and recipes packages are used for pre-processing data. I don’t know everything it can do, but I found ot is very useful to prevent leaking data from the test set into the train set, and to generate all the models to impute missing datas.<br />
* My double-lift chart generation function disloc() and lift-chart generation function get_lift_chart() finally look good.<br />
* This is my first tweedie, both using glm (package <code>statmod</code>) and xgboost.</p>
<p>TODO: learn <code>tuner</code> to find the best xgboost hyperparameters and tweedie parameters.</p>
<div id="libraries" class="section level1">
<h1>Libraries</h1>
<p>The data comes from the <code>insuranceData</code> package, described below in the data section.<br />
Data wrangling and plot is done using the <code>tidyverse</code> and <code>patchwork</code>, as usual.<br />
The <code>statmod</code> library is required to evaluate tweedie models using GLM.<br />
<code>modelr</code>is used for the add_predictions() function.<br />
<code>broom::tidy()</code>is used to get GLM coefficients in table format,<br />
<code>xgboost</code> is used to evaluate the gradient boosting model.<br />
<code>tictoc::tic() and tictoc::toc()</code> are used to measure evaluation time.<br />
Claus Wilke’s <code>cowplot::theme_cowplot()</code>, <code>dviz.supp::theme_dviz_hgrid()</code> and <code>colorblindr::scale_color_OkabeIto()</code> are used for to make my plots look better
<code>MLmetrics::NormalizedGini</code> for normalized gini coefficient.</p>
<pre class="r"><code># https://www.cybaea.net/Journal/2012/03/13/R-code-for-Chapter-2-of-Non_Life-Insurance-Pricing-with-GLM/
library(insuranceData) # for  dataCar  insurance data
library(tidyverse)  # pour la manipulation de données
library(statmod) #pour glm(family = tweedie)
library(modelr) # pour add_predictions()
library(broom) # pour afficher les coefficients
library(tidymodels)
library(xgboost)
library(tictoc)
library(dviz.supp) # devtools::install_github(&quot;clauswilke/dviz.supp&quot;)
library(colorblindr)  # devtools::install_github(&quot;clauswilke/colorblindr&quot;)
library(patchwork)
library(rsample)
library(yardstick)
library(recipes)
library(MLmetrics) # for normalized gini </code></pre>
</div>
<div id="data" class="section level1">
<h1>Data</h1>
<p>The dataCar data from the <code>insuranceData</code> package. It contains 67 856 one-year vehicle insurance policies taken out in 2004 or 2005. It originally came with the book <a href="http://www.businessandeconomics.mq.edu.au/our_departments/Applied_Finance_and_Actuarial_Studies/research/books/GLMsforInsuranceData">Generalized Linear Models for Insurance Data (2008)</a>.</p>
<p>The <code>exposure</code> variable represents the “number of year of exposure” and is used as the offset variable. It is bounded between 0 and 1.</p>
<p>Finally, the independent variables are as follow:</p>
<ul>
<li><code>veh_value</code>, the vehicle value in tens of thousand of dollars,<br />
</li>
<li><code>veh_body</code>, y vehicle body, coded as BUS CONVT COUPE HBACK HDTOP MCARA MIBUS PANVN RDSTR SEDAN STNWG TRUCK UTE,<br />
</li>
<li><code>veh_age</code>, 1 (youngest), 2, 3, 4,<br />
</li>
<li><code>gender</code>, a factor with levels F M,<br />
</li>
<li><code>area</code> a factor with levels A B C D E F,<br />
</li>
<li><code>agecat</code> 1 (youngest), 2, 3, 4, 5, 6</li>
</ul>
<p>The dollar amount of the claims is <code>claimcst0</code>. We will divide it by the exposure to obtain <code>annual_loss</code> which is the pure annual premium.</p>
<p>For the frequency (Poisosn) model, we will model clm (0 or 1) because the cost variables (claimcst0) represents the total cost, if you have multiple claims (numclaims&gt;1).</p>
</div>
<div id="pre-process-the-data-using-recipe" class="section level1">
<h1>Pre-process the data using recipe</h1>
<p>I’ve used a few tutorials and vignettes, including the following two:</p>
<p><a href="https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html" class="uri">https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html</a>
<a href="https://www.andrewaage.com/post/a-simple-solution-to-a-kaggle-competition-using-xgboost-and-recipes/" class="uri">https://www.andrewaage.com/post/a-simple-solution-to-a-kaggle-competition-using-xgboost-and-recipes/</a></p>
<p>I honestly havent done that much reading, but here is why I have adopted <code>recipes</code>. :<br />
* create dummies for xgboost in 1 line of code,<br />
* trainn knn models to impute all missing predictors in 1 line of code,<br />
* combine super rare categories into “other” in 1 line of code, (not done here, but all is needed is step_knnimpute(all_predictors()))</p>
<p>All while making sure that you don’t leak any test data into your train data. What’s not to like?</p>
<pre class="r"><code>

data(dataCar)

# claimcst0 = claim amount (0 if no claim)
# clm = 0 or 1 = has a claim yes/ no  
#  numclaims = number of claims  0 , 1 ,2 ,3 or 4).       
# we use clm because the corresponding dollar amount is for all claims combined.  
mydb &lt;- dataCar %&gt;% select(clm, claimcst0, exposure, veh_value, veh_body,
                           veh_age, gender, area, agecat) %&gt;% 
  mutate(annual_loss = claimcst0 / exposure)

set.seed(42)

split &lt;- rsample::initial_split(mydb, prop = 0.8)
train &lt;- rsample::training(split)
test &lt;- rsample::testing(split)

rec &lt;-  recipes::recipe(train %&gt;% select(annual_loss, clm, claimcst0,  exposure, veh_value,veh_body, veh_age, gender,area,agecat)) %&gt;% 
  recipes::update_role(everything(), new_role = &quot;predictor&quot;) %&gt;%
  recipes::update_role(annual_loss, new_role = &quot;outcome&quot;) %&gt;%
  recipes::update_role(clm, new_role = &quot;outcome&quot;) %&gt;%
  recipes::update_role(claimcst0, new_role = &quot;outcome&quot;) %&gt;%
  recipes::update_role(exposure, new_role = &quot;case weight&quot;) %&gt;%
  recipes::step_zv(recipes::all_predictors()) %&gt;%   # remove variable with all equal values
  recipes::step_other(recipes::all_predictors(), threshold = 0.01)  %&gt;%       # combine categories with less than 1% of observation
  recipes::step_dummy(recipes::all_nominal())  %&gt;% # convert to dummy for xgboost use
  check_missing(all_predictors()) ## break the bake function if any of the checked columns contains NA value

# Prepare the recipe and use juice/bake to get the data!
prepped_rec &lt;- prep(rec)
train &lt;- juice(prepped_rec)
test &lt;- bake(prepped_rec, new_data = test)</code></pre>
</div>
<div id="glms" class="section level1">
<h1>GLMs</h1>
<div id="tweedie-model-pure-premium" class="section level2">
<h2>Tweedie model (pure premium)</h2>
<p>We convert the dollar amount into an annual premium by dividing the dollar amount (claimst0) by the number of years of exposure (exposure).<br />
We weight each observation by the number of years of exposure.</p>
<p>The model isnt very impressive, with only agecat and the itnercept having a nice p-value..</p>
<pre class="r"><code>tweedie_fit &lt;- 
  glm(annual_loss ~ . -exposure -clm -claimcst0,
      family=tweedie(var.power=1.1, link.power=0),
      weights = exposure,
      data = train)

summary(tweedie_fit)
## 
## Call:
## glm(formula = annual_loss ~ . - exposure - clm - claimcst0, family = tweedie(var.power = 1.1, 
##     link.power = 0), data = train, weights = exposure)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -33.25  -15.06  -11.51   -7.36  505.94  
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      6.97758    0.75304   9.266  &lt; 2e-16 ***
## veh_age         -0.03772    0.05585  -0.675  0.49947    
## agecat          -0.13790    0.04209  -3.277  0.00105 ** 
## veh_value_other -0.07492    0.60543  -0.124  0.90151    
## veh_body_HBACK  -0.80225    0.41356  -1.940  0.05240 .  
## veh_body_HDTOP  -0.68656    0.52018  -1.320  0.18689    
## veh_body_MIBUS  -0.87444    0.71112  -1.230  0.21883    
## veh_body_PANVN  -0.62271    0.60152  -1.035  0.30057    
## veh_body_SEDAN  -0.93649    0.41221  -2.272  0.02310 *  
## veh_body_STNWG  -0.90493    0.41637  -2.173  0.02975 *  
## veh_body_TRUCK  -0.77203    0.51819  -1.490  0.13627    
## veh_body_UTE    -0.97021    0.46023  -2.108  0.03503 *  
## veh_body_other  -1.01044    1.05428  -0.958  0.33786    
## gender_M         0.15396    0.12186   1.263  0.20645    
## area_B           0.09286    0.17961   0.517  0.60515    
## area_C           0.10631    0.16226   0.655  0.51237    
## area_D          -0.12412    0.22651  -0.548  0.58373    
## area_E           0.24128    0.22645   1.065  0.28665    
## area_F           0.41737    0.25105   1.663  0.09641 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Tweedie family taken to be 13678.45)
## 
##     Null deviance: 25990220  on 54284  degrees of freedom
## Residual deviance: 25646806  on 54266  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 9</code></pre>
</div>
<div id="poisson-model-frequency" class="section level2">
<h2>Poisson model (frequency)</h2>
<p>We model the presence of a claim (clm) and use the log(exposure) as an offset.</p>
<pre class="r"><code>
poisson_fit &lt;-
  glm(clm ~ . -annual_loss -exposure -claimcst0 ,
      family = poisson(link = &quot;log&quot;),
      offset = log(exposure),
      data = train)

#broom::tidy(poisson_fit)
summary(poisson_fit)
## 
## Call:
## glm(formula = clm ~ . - annual_loss - exposure - claimcst0, family = poisson(link = &quot;log&quot;), 
##     data = train, offset = log(exposure))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.7375  -0.4364  -0.3346  -0.2142   3.5503  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -1.054539   0.235626  -4.475 7.62e-06 ***
## veh_age         -0.076200   0.015751  -4.838 1.31e-06 ***
## agecat          -0.085522   0.011886  -7.195 6.24e-13 ***
## veh_value_other  0.080809   0.186824   0.433 0.665348    
## veh_body_HBACK  -0.516787   0.135851  -3.804 0.000142 ***
## veh_body_HDTOP  -0.284150   0.163919  -1.733 0.083012 .  
## veh_body_MIBUS  -0.564570   0.220348  -2.562 0.010402 *  
## veh_body_PANVN  -0.400556   0.193340  -2.072 0.038287 *  
## veh_body_SEDAN  -0.479023   0.135038  -3.547 0.000389 ***
## veh_body_STNWG  -0.399992   0.136005  -2.941 0.003271 ** 
## veh_body_TRUCK  -0.509168   0.168834  -3.016 0.002563 ** 
## veh_body_UTE    -0.669788   0.150302  -4.456 8.34e-06 ***
## veh_body_other  -0.103584   0.254960  -0.406 0.684540    
## gender_M        -0.003482   0.034668  -0.100 0.919994    
## area_B           0.064939   0.049370   1.315 0.188387    
## area_C           0.018744   0.045098   0.416 0.677686    
## area_D          -0.122612   0.061803  -1.984 0.047266 *  
## area_E          -0.008238   0.066679  -0.124 0.901675    
## area_F          -0.001512   0.078559  -0.019 0.984646    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 19004  on 54284  degrees of freedom
## Residual deviance: 18885  on 54266  degrees of freedom
## AIC: 26279
## 
## Number of Fisher Scoring iterations: 6</code></pre>
</div>
<div id="gamma-model" class="section level2">
<h2>Gamma model</h2>
<p>For the 7% of policies with a claim, we model the dollar amount of claims (claimcst0)</p>
<pre class="r"><code>gamma_fit &lt;-
  glm(claimcst0 ~ . -annual_loss -exposure -clm ,
      data = train %&gt;% filter( claimcst0 &gt; 0),
      family = Gamma(&quot;log&quot;))

#broom::tidy(gamma_fit) 
summary(gamma_fit)
## 
## Call:
## glm(formula = claimcst0 ~ . - annual_loss - exposure - clm, family = Gamma(&quot;log&quot;), 
##     data = train %&gt;% filter(claimcst0 &gt; 0))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9490  -1.3477  -0.7828   0.0977   6.9485  
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      7.987519   0.391035  20.427  &lt; 2e-16 ***
## veh_age          0.049425   0.026406   1.872 0.061329 .  
## agecat          -0.052324   0.019640  -2.664 0.007752 ** 
## veh_value_other -0.141215   0.310703  -0.455 0.649494    
## veh_body_HBACK  -0.271797   0.225970  -1.203 0.229131    
## veh_body_HDTOP  -0.405127   0.272327  -1.488 0.136930    
## veh_body_MIBUS  -0.331422   0.366175  -0.905 0.365476    
## veh_body_PANVN  -0.191639   0.321404  -0.596 0.551042    
## veh_body_SEDAN  -0.442009   0.224734  -1.967 0.049281 *  
## veh_body_STNWG  -0.508804   0.226379  -2.248 0.024662 *  
## veh_body_TRUCK  -0.236181   0.281025  -0.840 0.400723    
## veh_body_UTE    -0.290821   0.250266  -1.162 0.245291    
## veh_body_other  -0.930757   0.423793  -2.196 0.028136 *  
## gender_M         0.151658   0.057777   2.625 0.008704 ** 
## area_B           0.009213   0.082098   0.112 0.910657    
## area_C           0.086454   0.075065   1.152 0.249507    
## area_D          -0.018480   0.103047  -0.179 0.857682    
## area_E           0.247247   0.111179   2.224 0.026219 *  
## area_F           0.432186   0.130822   3.304 0.000964 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Gamma family taken to be 2.757837)
## 
##     Null deviance: 5705.3  on 3677  degrees of freedom
## Residual deviance: 5556.3  on 3659  degrees of freedom
## AIC: 62848
## 
## Number of Fisher Scoring iterations: 7</code></pre>
</div>
</div>
<div id="xgboost" class="section level1">
<h1>XGBoost</h1>
<div id="xgboost-tweedie-model" class="section level2">
<h2>XGBoost Tweedie Model</h2>
<pre class="r"><code>xgtrain_tweedie &lt;- xgb.DMatrix(as.matrix(train %&gt;% select(-annual_loss ,-clm, - claimcst0, -exposure )),  
                               label = train$annual_loss,
                               weight = train$exposure
)


xgtest_tweedie &lt;- xgb.DMatrix(as.matrix(test %&gt;% select(-annual_loss ,-clm, - claimcst0, -exposure )),  
                              label = test$annual_loss,
                              weight = test$exposure
)



params &lt;-list(
  booster = &quot;gbtree&quot;,
  objective = &#39;reg:tweedie&#39;,
  eval_metric = &quot;tweedie-nloglik@1.1&quot;,
  tweedie_variance_power = 1.1,
  gamma = 0,
  max_depth = 4,
  eta = 0.01,
  min_child_weight = 3,
  subsample = 0.8,
  colsample_bytree = 0.8,
  tree_method = &quot;hist&quot;
)

xgcv_tweedie &lt;- xgb.cv(
  params = params,
  data = xgtrain_tweedie,
  nround = 1000,
  nfold=  5,
  showsd = TRUE,
  early_stopping_rounds = 50,
  verbose = 0)

xgcv_tweedie$best_iteration
## [1] 319


# xgb.cv doesnt ouput any model -- we need a model to predict test dataset
xgmodel_tweedie &lt;- xgboost::xgb.train(
  data = xgtrain_tweedie,
  params = params,
  nrounds = xgcv_tweedie$best_iteration,
  nthread = parallel::detectCores() - 1
)</code></pre>
</div>
<div id="xgboost-poisson-model" class="section level2">
<h2>XGBoost Poisson model</h2>
<pre class="r"><code>xgtrain_poisson &lt;- xgb.DMatrix(as.matrix(train %&gt;% select(-annual_loss ,-clm, - claimcst0, -exposure )),  
                               label = train$clm
)


xgtest_poisson &lt;- xgb.DMatrix(as.matrix(test %&gt;% select(-annual_loss ,-clm, - claimcst0, -exposure )),  
                              label = test$clm
)

setinfo(xgtrain_poisson,&quot;base_margin&quot;, 
        train %&gt;% pull(exposure) %&gt;% log() )
## [1] TRUE

setinfo(xgtest_poisson,&quot;base_margin&quot;, 
        train %&gt;% pull(exposure) %&gt;% log() )
## [1] TRUE

params &lt;-list(
  booster = &quot;gbtree&quot;,
  objective = &#39;count:poisson&#39;, 
  eval_metric = &quot;poisson-nloglik&quot;,
  gamma = 0,
  max_depth = 4,
  eta = 0.05,
  min_child_weight = 3,
  subsample = 0.8,
  colsample_bytree = 0.8,
  tree_method = &quot;hist&quot;
)

xgcv_poisson &lt;- xgb.cv(
  params = params,
  data = xgtrain_poisson,
  nround = 1000,
  nfold=  5,
  showsd = TRUE,
  early_stopping_rounds = 50,
  verbose = 0)

xgcv_poisson$best_iteration
## [1] 212


# xgb.cv doesnt ouput any model -- we need a model to predict test dataset
xgmodel_poisson &lt;- xgboost::xgb.train(
  data = xgtrain_poisson,
  params = params,
  nrounds = xgcv_poisson$best_iteration,
  nthread = parallel::detectCores() - 1
)</code></pre>
</div>
<div id="xgboost-gamma-model" class="section level2">
<h2>XGBoost Gamma model</h2>
<p>Gamma model is only train on policy with claims</p>
<pre class="r"><code>xgtrain_gamma &lt;- xgb.DMatrix(as.matrix(train %&gt;% filter(claimcst0 &gt; 0) %&gt;%select(-annual_loss ,-clm, - claimcst0, -exposure )),  
                             label = train %&gt;% filter(claimcst0 &gt; 0) %&gt;% pull(claimcst0)
)

xgtest_gamma &lt;- xgb.DMatrix(as.matrix(test %&gt;% select(-annual_loss ,-clm, - claimcst0, -exposure )),  
                            label = test$claimcst0
)

params &lt;-list(
  booster = &quot;gbtree&quot;,
  objective = &#39;reg:gamma&#39;,
  gamma = 0,
  max_depth = 4,
  eta = 0.05,
  min_child_weight = 3,
  subsample = 0.8,
  colsample_bytree = 0.8,
  tree_method = &quot;hist&quot;
)

xgcv_gamma &lt;- xgb.cv(
  params = params,
  data = xgtrain_gamma,
  nround = 1000,
  nfold=  5,
  showsd = TRUE,
  early_stopping_rounds = 50,
  verbose = 0)

xgcv_gamma$best_iteration
## [1] 222


# xgb.cv doesnt ouput any model -- we need a model to predict test dataset
xgmodel_gamma &lt;- xgboost::xgb.train(
  data = xgtrain_gamma,
  params = params,
  nrounds = xgcv_gamma$best_iteration,
  nthread = parallel::detectCores() - 1
)</code></pre>
</div>
</div>
<div id="functions-lift-double-lift" class="section level1">
<h1>Functions lift &amp; double lift</h1>
<pre class="r"><code># double lift charts
#&#39; @title disloc()
#&#39;
#&#39; @description Cette fonction crée un tableau et une double lift chart
#&#39; @param data data.frame  source
#&#39; @param pred1 prediction of first model
#&#39; @param pred1 prediction of second model
#&#39; @param expo exposure var
#&#39; @param obs observed result
#&#39; @param nb nombre de quantils créés
#&#39; @param obs_lab Label pour la valeur observée dans le graphique
#&#39; @param pred1_lab Label pour la première prédiction dans le graphique
#&#39; @param pred2_lab Label pour la deuxième prédiction dans le graphique
#&#39; @param x_label Label pour la valeur réalisée dans le graphique
#&#39; @param y_label Label pour la valeur réalisée dans le graphique
#&#39; @param y_format Fonction utilisée pour formater l&#39;axe des y dans le graphique (par exemple percent_format() ou dollar_format() du package scales)
#&#39; @export

disloc &lt;- function(data, pred1, pred2, expo, obs, nb = 10,
                   obs_lab = &quot;&quot;,
                   pred1_lab = &quot;&quot;, pred2_lab = &quot;&quot;,
                   x_label = &quot;&quot;,
                   y_label= &quot;sinistralité&quot;,
                   y_format = scales::number_format(accuracy = 1,  big.mark = &quot; &quot;, decimal.mark = &quot;,&quot;)
) {
  # obligé de mettre les variables dans un enquo pour pouvoir les utiliser dans dplyr
  
  pred1_var &lt;- enquo(pred1)
  pred2_var &lt;- enquo(pred2)
  expo_var &lt;- enquo(expo)
  obs_var &lt;- enquo(obs)
  
  
  pred1_name &lt;- quo_name(pred1_var)
  pred2_name &lt;- quo_name(pred2_var)
  obs_name &lt;- quo_name(obs_var)
  
  
  if (pred1_lab ==&quot;&quot;) {pred1_lab &lt;- pred1_name}
  if (pred2_lab ==&quot;&quot;) {pred2_lab &lt;- pred2_name}
  if (obs_lab ==&quot;&quot;) {obs_lab &lt;- obs_name}
  
  if (x_label == &quot;&quot;){ x_label &lt;- paste0(&quot;ratio entre les prédictions &quot;, pred1_lab, &quot; / &quot;, pred2_lab)}
  
  # création de la comparaison entre les deux pred
  dd &lt;- data %&gt;%
    mutate(ratio = !!pred1_var / !!pred2_var) %&gt;%
    filter(!!expo_var &gt; 0) %&gt;%
    drop_na()
  
  # constitution des buckets de poids égaux
  dd &lt;- dd %&gt;% add_equal_weight_group(
    sort_by = ratio,
    expo = !!expo_var, 
    group_variable_name = &quot;groupe&quot;,
    nb = nb
  )
  
  # comparaison sur ces buckets
  dd &lt;- full_join(
    dd %&gt;% group_by(groupe) %&gt;%
      summarise(
        ratio_moyen = mean(ratio),
        ratio_min = min(ratio),
        ratio_max = max(ratio)
      ),
    dd %&gt;% group_by(groupe) %&gt;%
      summarise_at(
        funs(sum(.) / sum(!!expo_var)),
        .vars = vars(!!obs_var, !!pred1_var, !!pred2_var)
      ) %&gt;%
      ungroup,
    by = &quot;groupe&quot;
  )
  
  # création des labels
  dd &lt;- dd %&gt;%
    mutate(labs = paste0(&quot;[&quot;, round(ratio_min, 2), &quot;, &quot;, round(ratio_max, 2), &quot;]&quot;))
  
  # graphe
  plotdata &lt;-
    dd %&gt;%
    gather(key, variable, !!obs_var, !!pred1_var, !!pred2_var) %&gt;%
    ## Pas optimal mais je ne trouve pas mieux...
    mutate(key = case_when(
      key == obs_name ~ obs_lab,
      key == pred1_name ~ pred1_lab,
      key == pred2_name ~ pred2_lab
    )) %&gt;%
    mutate(key = factor(key, levels = c(obs_lab, pred1_lab, pred2_lab), ordered = TRUE))
  
  pl &lt;- plotdata %&gt;%
    ggplot(aes(ratio_moyen, variable, color = key, linetype = key)) +
    cowplot::theme_cowplot() +
    cowplot::background_grid()+
    colorblindr::scale_color_OkabeIto( ) + 
    
    scale_x_continuous(breaks = scales::pretty_breaks())+
    geom_line() +
    geom_point() +
    scale_x_continuous(breaks = dd$ratio_moyen, labels = dd$labs) +
    scale_y_continuous(breaks = scales::pretty_breaks() )+  
    labs(
      x = x_label,
      y = y_label
    )+
    theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)) #+
  
  
  # écart au réalisé, pondéré
  ecart &lt;- dd %&gt;%
    mutate(poids = abs(1 - ratio_moyen)) %&gt;%
    summarise_at(
      vars(!!pred1_var, !!pred2_var),
      funs(weighted.mean((. - !!obs_var)^2, w = poids) %&gt;% sqrt())
    ) %&gt;% summarise(ratio_distance = !!pred2_var / !!pred1_var) %&gt;%
    as.numeric()
  
  list(
    graphe = pl,
    ecart = ecart,
    tableau = dd
  )
}

#&#39; @title add_equal_weight_group()
#&#39;
#&#39; @description Cette fonction crée des groupe (quantiles) avec le nombre nombre total d&#39;exposition.
#&#39; @param table data.frame  source
#&#39; @param sort_by Variable utilisée pour trier les observations.
#&#39; @param expo Exposition (utilisée pour créer des quantiles de la même taille.  Si NULL, l&#39;exposition est égale pour toutes les observations) (Défault = NULL).
#&#39; @param nb Nombre de quantiles crées (défaut = 10)
#&#39; @param group_variable_name Nom de la variable de groupes créée
#&#39; @export


add_equal_weight_group &lt;- function(table, sort_by, expo = NULL, group_variable_name = &quot;groupe&quot;, nb = 10) {
  sort_by_var &lt;- enquo(sort_by)
  groupe_variable_name_var &lt;- enquo(group_variable_name)
  
  if (!(missing(expo))){ # https://stackoverflow.com/questions/48504942/testing-a-function-that-uses-enquo-for-a-null-parameter
    
    expo_var &lt;- enquo(expo)
    
    total &lt;- table %&gt;% pull(!!expo_var) %&gt;% sum
    br &lt;- seq(0, total, length.out = nb + 1) %&gt;% head(-1) %&gt;% c(Inf) %&gt;% unique
    table %&gt;%
      arrange(!!sort_by_var) %&gt;%
      mutate(cumExpo = cumsum(!!expo_var)) %&gt;%
      mutate(!!group_variable_name := cut(cumExpo, breaks = br, ordered_result = TRUE, include.lowest = TRUE) %&gt;% as.numeric) %&gt;%
      select(-cumExpo)
  } else {
    total &lt;- nrow(table)
    br &lt;- seq(0, total, length.out = nb + 1) %&gt;% head(-1) %&gt;% c(Inf) %&gt;% unique
    table %&gt;%
      arrange(!!sort_by_var) %&gt;%
      mutate(cumExpo = row_number()) %&gt;%
      mutate(!!group_variable_name := cut(cumExpo, breaks = br, ordered_result = TRUE, include.lowest = TRUE) %&gt;% as.numeric) %&gt;%
      select(-cumExpo)
  }
}

get_lift_chart_data &lt;- function(
  data, 
  sort_by,
  pred, 
  expo, 
  obs, 
  nb = 10) {
  
  pred_var &lt;- enquo(pred)
  sort_by_var &lt;- enquo(sort_by)
  expo_var &lt;- enquo(expo)
  obs_var &lt;- enquo(obs)
  
  
  pred_name &lt;- quo_name(pred_var)
  sort_by_name &lt;- quo_name(sort_by_var)
  obs_name &lt;- quo_name(obs_var)
  
  # constitution des buckets de poids égaux
  dd &lt;- data %&gt;% add_equal_weight_group(
    sort_by = !!sort_by_var,
    expo = !!expo_var, 
    group_variable_name = &quot;groupe&quot;,
    nb = nb
  )
  
  # comparaison sur ces buckets
  dd &lt;- full_join(
    dd %&gt;% 
      group_by(groupe) %&gt;%
      summarise(
        exposure = sum(!!expo_var),
        sort_by_moyen = mean(!!sort_by_var),
        sort_by_min = min(!!sort_by_var),
        sort_by_max = max(!!sort_by_var)
      ) %&gt;%
      ungroup(),
    dd %&gt;% 
      group_by(groupe) %&gt;%
      summarise_at(
        funs(sum(.) / sum(!!expo_var)),
        .vars = vars(!!obs_var, !!pred_var)
      ) %&gt;%
      ungroup,
    by = &quot;groupe&quot;
  )
  
  # création des labels
  dd &lt;- dd %&gt;%
    mutate(labs = paste0(&quot;[&quot;, round(sort_by_min, 2), &quot;, &quot;, round(sort_by_max, 2), &quot;]&quot;))
  
}

get_lift_chart &lt;- function(data, 
                           sort_by,
                           pred, 
                           expo, 
                           obs, 
                           nb){
  
  pred_var &lt;- enquo(pred)
  sort_by_var &lt;- enquo(sort_by)
  expo_var &lt;- enquo(expo)
  obs_var &lt;- enquo(obs)
  
  lift_data &lt;- get_lift_chart_data(
    data = data, 
    sort_by = !!sort_by_var,
    pred = !!pred_var, 
    expo = !!expo_var, 
    obs = !!obs_var, 
    nb = 10)
  
  p1 &lt;- lift_data %&gt;% 
    mutate(groupe = as.factor(groupe)) %&gt;%
    select(groupe, labs, !!pred_var, !!obs_var) %&gt;%
    gather(key = type, value = average, !!pred_var, !!obs_var)  %&gt;% 
    ggplot(aes(x= groupe, y = average, color =type, group = type)) +
    geom_line() +
    geom_point()+
    cowplot::theme_half_open() +
    cowplot::background_grid() +
    colorblindr::scale_color_OkabeIto( ) + 
    # scale_y_continuous(
    #   breaks  = scales::pretty_breaks()
    # )+
    theme(
      legend.position = c(0.1, 0.8),
      axis.title.x=element_blank(),
      axis.text.x=element_blank(),
      axis.ticks.x=element_blank()
    )
  
  p2 &lt;- lift_data %&gt;% 
    mutate(groupe = as.factor(groupe)) %&gt;%
    select(groupe, labs, exposure) %&gt;%
    ggplot(aes(x= groupe, y = exposure)) +
    geom_col() +
    cowplot::theme_half_open() +
    cowplot::background_grid() +
    colorblindr::scale_color_OkabeIto( )# + 
    # scale_y_continuous(
    #   expand = c(0,0),
    #   breaks  = scales::breaks_pretty(3)
    # )
  
  return(p1 / p2 +   plot_layout(heights = c(3, 1)))
}</code></pre>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<p>Add predictions to the test dataset.<br />
Quick check: the means for predictions of frequency and total cost don’t appear to be way off</p>
<pre class="r"><code>test_w_preds &lt;- test %&gt;%
  add_predictions(&quot;pred_annual_loss_glm&quot;, model = tweedie_fit, type = &quot;response&quot;) %&gt;%
  mutate(pred_loss_glm = pred_annual_loss_glm * exposure) %&gt;%
  add_predictions(&quot;pred_poisson_glm&quot;, model = poisson_fit, type = &quot;response&quot;) %&gt;%
  add_predictions(&quot;pred_gamma_glm&quot;, model = gamma_fit, type = &quot;response&quot;)  %&gt;%
  mutate(pred_frequency_severity_glm = pred_poisson_glm * pred_gamma_glm,
         pred_annual_loss_xgboost = predict(xgmodel_tweedie, xgtest_tweedie),
         pred_loss_xgboost = pred_annual_loss_xgboost * exposure,
         pred_poisson_xgboost =  predict(xgmodel_poisson, xgtest_poisson),
         pred_gamma_xgboost =  predict(xgmodel_gamma, xgtest_gamma),
         pred_frequency_severity_xgboost = pred_poisson_xgboost * pred_gamma_xgboost,
         pred_annual_freq_glm = pred_poisson_glm / exposure,
         pred_annual_freq_xgboost = pred_poisson_xgboost / exposure,)


test_w_preds %&gt;%
  select(claimcst0, pred_loss_glm, pred_frequency_severity_glm, pred_loss_xgboost, pred_frequency_severity_xgboost,
         clm, pred_poisson_glm, pred_poisson_xgboost) %&gt;% 
  skimr::skim() %&gt;%
  knitr::kable(digits = 3L)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">skim_type</th>
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">numeric.mean</th>
<th align="right">numeric.sd</th>
<th align="right">numeric.p0</th>
<th align="right">numeric.p25</th>
<th align="right">numeric.p50</th>
<th align="right">numeric.p75</th>
<th align="right">numeric.p100</th>
<th align="left">numeric.hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">claimcst0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">161.098</td>
<td align="right">1283.589</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">55922.130</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">pred_loss_glm</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">131.153</td>
<td align="right">94.004</td>
<td align="right">0.382</td>
<td align="right">56.520</td>
<td align="right">117.134</td>
<td align="right">187.708</td>
<td align="right">786.876</td>
<td align="left">▇▃▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">pred_frequency_severity_glm</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">131.292</td>
<td align="right">94.274</td>
<td align="right">0.379</td>
<td align="right">56.559</td>
<td align="right">117.243</td>
<td align="right">187.708</td>
<td align="right">770.042</td>
<td align="left">▇▃▁▁▁</td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">pred_loss_xgboost</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">120.634</td>
<td align="right">98.135</td>
<td align="right">0.278</td>
<td align="right">51.604</td>
<td align="right">106.094</td>
<td align="right">171.248</td>
<td align="right">3875.319</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">pred_frequency_severity_xgboost</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">130.627</td>
<td align="right">34.040</td>
<td align="right">43.804</td>
<td align="right">108.232</td>
<td align="right">128.887</td>
<td align="right">145.492</td>
<td align="right">520.413</td>
<td align="left">▇▃▁▁▁</td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">clm</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.070</td>
<td align="right">0.255</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">1.000</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">pred_poisson_glm</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.067</td>
<td align="right">0.044</td>
<td align="right">0.000</td>
<td align="right">0.030</td>
<td align="right">0.063</td>
<td align="right">0.100</td>
<td align="right">0.286</td>
<td align="left">▇▆▃▁▁</td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">pred_poisson_xgboost</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.075</td>
<td align="right">0.013</td>
<td align="right">0.036</td>
<td align="right">0.066</td>
<td align="right">0.075</td>
<td align="right">0.082</td>
<td align="right">0.165</td>
<td align="left">▂▇▂▁▁</td>
</tr>
</tbody>
</table>
<div id="normalized-gini" class="section level2">
<h2>Normalized Gini</h2>
<p>The GLM model systematically perform better than the XGB models.<br />
The XGB combination of Frequency and Severity models has a Normalized Gini 4x smaller than it’s GLM counterpart.</p>
<p>We will really need to do this exercise again with a larger dataset.</p>
<pre class="r"><code>Poisson_GLM = MLmetrics::NormalizedGini(test_w_preds$clm, test_w_preds$pred_poisson_glm)
Poisson_XGB = MLmetrics::NormalizedGini(test_w_preds$clm, test_w_preds$pred_poisson_xgboost)
Tweedie_GLM = MLmetrics::NormalizedGini(test_w_preds$claimcst0, test_w_preds$pred_loss_glm)
Tweedie_XGB = MLmetrics::NormalizedGini(test_w_preds$claimcst0, test_w_preds$pred_loss_xgboost)
FreqSev_GLM = MLmetrics::NormalizedGini(test_w_preds$claimcst0, test_w_preds$pred_frequency_severity_glm)
FreqSev_XGB = MLmetrics::NormalizedGini(test_w_preds$claimcst0, test_w_preds$pred_frequency_severity_xgboost    )


tibble(Poisson_GLM,
       Poisson_XGB,
       Tweedie_GLM,
       Tweedie_XGB,
       FreqSev_GLM,
       FreqSev_XGB) %&gt;%
  gather(key = model, value = NormalizedGini) %&gt;%
  knitr::kable(digits = 3L)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">NormalizedGini</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Poisson_GLM</td>
<td align="right">0.036</td>
</tr>
<tr class="even">
<td align="left">Poisson_XGB</td>
<td align="right">0.018</td>
</tr>
<tr class="odd">
<td align="left">Tweedie_GLM</td>
<td align="right">0.041</td>
</tr>
<tr class="even">
<td align="left">Tweedie_XGB</td>
<td align="right">0.042</td>
</tr>
<tr class="odd">
<td align="left">FreqSev_GLM</td>
<td align="right">0.041</td>
</tr>
<tr class="even">
<td align="left">FreqSev_XGB</td>
<td align="right">0.026</td>
</tr>
</tbody>
</table>
</div>
<div id="lift-charts-of-select-models" class="section level2">
<h2>Lift charts of select models</h2>
<div id="tweedie-glm-pure-premium" class="section level3">
<h3>Tweedie GLM (pure premium)</h3>
<pre class="r"><code>get_lift_chart(
  data = test_w_preds,
  sort_by= pred_annual_loss_glm, 
  pred = pred_loss_glm, 
  obs = claimcst0, 
  expo  = exposure )</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-11-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="tweedie-xgboost-pure-premium" class="section level3">
<h3>Tweedie XGBoost (pure premium)</h3>
<pre class="r"><code>
get_lift_chart(
  data = test_w_preds,
  sort_by= pred_annual_loss_xgboost, 
  pred = pred_loss_xgboost, 
  obs = claimcst0, 
  expo  = exposure )</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-12-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="poisson-glm-frequency" class="section level3">
<h3>Poisson GLM (frequency)</h3>
<pre class="r"><code>get_lift_chart(
  data = test_w_preds, 
  sort_by= pred_annual_freq_glm, 
  pred = pred_poisson_glm,
  obs = clm, 
  expo  = exposure )</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-13-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="poisson-xgboost-frequency" class="section level3">
<h3>Poisson XGBoost (frequency)</h3>
<pre class="r"><code>get_lift_chart(
  data = test_w_preds, 
  sort_by= pred_annual_freq_xgboost, 
  pred = pred_poisson_xgboost,
  obs = clm, 
  expo  = exposure )</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-14-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="a-test-extract-individual-plot-from-a-patchwork-object-it-works" class="section level3">
<h3>A test: extract individual plot from a patchwork object (it works!)</h3>
<pre class="r"><code>gaa2 &lt;- get_lift_chart(
  data = test_w_preds, 
  sort_by= pred_annual_freq_glm, 
  pred = pred_poisson_glm,
  obs = clm, 
  expo  = exposure )

gaa1 &lt;- get_lift_chart(
  data = test_w_preds, 
  sort_by= pred_annual_freq_xgboost, 
  pred = pred_poisson_xgboost,
  obs = clm, 
  expo  = exposure )

(gaa1[[1]] + gaa2[[1]]) /(gaa1[[2]] + gaa2[[2]])  +    plot_layout(heights = c(3, 1))</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-15-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="double-lift-chart" class="section level2">
<h2>Double lift chart</h2>
<div id="glm-tweedie-vs-glm-poisson-gamma" class="section level3">
<h3>GLM Tweedie vs GLM Poisson * Gamma</h3>
<p>GLM Tweedie ( power = 1.1) appears similar to than Poisson * Gamma .</p>
<pre class="r"><code>disloc(data = test_w_preds, 
       pred1 = pred_loss_glm, 
       pred2 = pred_frequency_severity_glm, 
       expo = exposure, 
       obs = claimcst0 ,
       y_label = &quot;coût moyen ($)&quot;
) %&gt;% .$graphe</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-16-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="glm-tweedie-vs-xgb-tweedie" class="section level3">
<h3>GLM Tweedie vs XGB Tweedie</h3>
<p>Why is the xgboost tweedie doing so poorly vs GLM tweedie?<br />
partial answer: xgboost is out of fold, glm can try to overfit.. but it really doesnt have that many variables to overfit on.</p>
<pre class="r"><code>
disloc(data = test_w_preds, 
       pred1 = pred_loss_xgboost , 
       pred2 = pred_loss_glm, 
       expo = exposure, 
       obs = claimcst0 ,
       y_label = &quot;coût moyen ($)&quot;
) %&gt;% .$graphe</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-17-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="glm-poissongamma-vs-xgb-poissongamma" class="section level3">
<h3>GLM Poisson<em>Gamma vs XGB Poisson</em>Gamma</h3>
<pre class="r"><code>disloc(data = test_w_preds, 
       pred1 = pred_frequency_severity_glm, 
       pred2 = pred_frequency_severity_xgboost, 
       expo = exposure, 
       obs = claimcst0 ,
       y_label = &quot;coût moyen ($)&quot;
) %&gt;% .$graphe</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-18-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
