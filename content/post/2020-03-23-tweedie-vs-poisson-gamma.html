---
title: Tweedie vs Poisson * Gamma
author: Simon
date: '2020-03-23'
slug: tweedie-vs-poisson-gamma
categories:
  - R
  - insurnace
tags: []
keywords:
  - tech
---



<p>I’m building my first tweedie model, and I’m finally trying the {recipes} package.</p>
<p>We will try to predict the pure premium of car insurance policy. This can be done directly with a tweedie model, or by multiplying two separates models: a frequency (Poisson) and a severity (Gamma) model. We wil be using “lift charts” and “double lift charts” to evaluate the model performance .</p>
<p>Here’s is the plan:</p>
<ul>
<li>Pre-process the train and test data using <code>recipes</code>.<br />
</li>
<li>Estimate the pure premium directly using a Tweedie model (93% of <code>claimcst0</code>values are 0$, the others are the claims dollar amount).<br />
</li>
<li>Estimate the frequency of claims using a Poisson model (93% of values <code>clm</code>values are 0, the other are equal to 1).</li>
<li>Estimate the severity of claims using a Gamma model (estimate the <code>claimcst0</code> value for the 7% that is above 0$)<br />
</li>
<li>All three models are estimated using a GLM and a GBM approach.<br />
</li>
<li>Models are compared using three approaches :
<ul>
<li>Normalized Gini Index,</li>
<li>Lift charts,<br />
</li>
<li>Double-lift charts.</li>
</ul></li>
</ul>
<p>More details <a href="https://www.casact.org/education/rpm/2015/handouts/Paper_4034_handout_2419_0.pdf">about these charts here</a>. What I call “lift chart” is named “simple quantile plots” in the pdf.</p>
<p>tl;dr: <code>recipes</code> is AWESOME. Also, both glm and xgboost models all perform poorly. <a href="https://www.simoncoulombe.com/2020/03/tweedue-and-recipes-part-2-kaggle-data/">I made a new post using 6M observatiosn instead of 67k, and it helps.</a></p>
<p>Reasons I will come back to this post for code snippets:<br />
* the rsample, yardstick and recipes packages are used for pre-processing data. I don’t know everything it can do, but I found ot is very useful to prevent leaking data from the test set into the train set, and to generate all the models to impute missing datas.<br />
* My double-lift chart generation function disloc() and lift-chart generation function get_lift_chart() finally look good.<br />
* This is my first tweedie, both using glm (package <code>statmod</code>) and xgboost.</p>
<p>TODO: learn <code>tuner</code> to find the best xgboost hyperparameters and tweedie parameters.</p>
<div id="libraries" class="section level1">
<h1>Libraries</h1>
<p>The data comes from the <code>insuranceData</code> package, described below in the data section.<br />
Data wrangling and plot is done using the <code>tidyverse</code> and <code>patchwork</code>, as usual.<br />
The <code>statmod</code> library is required to evaluate tweedie models using GLM.<br />
<code>modelr</code>is used for the add_predictions() function.<br />
<code>broom::tidy()</code>is used to get GLM coefficients in table format,<br />
<code>xgboost</code> is used to evaluate the gradient boosting model.<br />
<code>tictoc::tic() and tictoc::toc()</code> are used to measure evaluation time.<br />
Claus Wilke’s <code>cowplot::theme_cowplot()</code>, <code>dviz.supp::theme_dviz_hgrid()</code> and <code>colorblindr::scale_color_OkabeIto()</code> are used for to make my plots look better
<code>MLmetrics::NormalizedGini</code> for normalized gini coefficient.</p>
<pre class="r"><code># https://www.cybaea.net/Journal/2012/03/13/R-code-for-Chapter-2-of-Non_Life-Insurance-Pricing-with-GLM/
library(insuranceData) # for  dataCar  insurance data
library(tidyverse)  # pour la manipulation de données
library(statmod) #pour glm(family = tweedie)
library(modelr) # pour add_predictions()
library(broom) # pour afficher les coefficients
library(tidymodels)
library(xgboost)
library(tictoc)
library(dviz.supp) # devtools::install_github(&quot;clauswilke/dviz.supp&quot;)
library(colorblindr)  # devtools::install_github(&quot;clauswilke/colorblindr&quot;)
library(patchwork)
library(rsample)
library(yardstick)
library(recipes)
library(MLmetrics) # for normalized gini </code></pre>
</div>
<div id="data" class="section level1">
<h1>Data</h1>
<p>The dataCar data from the <code>insuranceData</code> package. It contains 67 856 one-year vehicle insurance policies taken out in 2004 or 2005. It originally came with the book <a href="http://www.businessandeconomics.mq.edu.au/our_departments/Applied_Finance_and_Actuarial_Studies/research/books/GLMsforInsuranceData">Generalized Linear Models for Insurance Data (2008)</a>.</p>
<p>The <code>exposure</code> variable represents the “number of year of exposure” and is used as the offset variable. It is bounded between 0 and 1.</p>
<p>Finally, the independent variables are as follow:</p>
<ul>
<li><code>veh_value</code>, the vehicle value in tens of thousand of dollars,<br />
</li>
<li><code>veh_body</code>, y vehicle body, coded as BUS CONVT COUPE HBACK HDTOP MCARA MIBUS PANVN RDSTR SEDAN STNWG TRUCK UTE,<br />
</li>
<li><code>veh_age</code>, 1 (youngest), 2, 3, 4,<br />
</li>
<li><code>gender</code>, a factor with levels F M,<br />
</li>
<li><code>area</code> a factor with levels A B C D E F,<br />
</li>
<li><code>agecat</code> 1 (youngest), 2, 3, 4, 5, 6</li>
</ul>
<p>The dollar amount of the claims is <code>claimcst0</code>. We will divide it by the exposure to obtain <code>annual_loss</code> which is the pure annual premium.</p>
<p>For the frequency (Poisosn) model, we will model clm (0 or 1) because the cost variables (claimcst0) represents the total cost, if you have multiple claims (numclaims&gt;1).</p>
</div>
<div id="pre-process-the-data-using-recipe" class="section level1">
<h1>Pre-process the data using recipe</h1>
<p>I’ve used a few tutorials and vignettes, including the following two:</p>
<p><a href="https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html" class="uri">https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html</a>
<a href="https://www.andrewaage.com/post/a-simple-solution-to-a-kaggle-competition-using-xgboost-and-recipes/" class="uri">https://www.andrewaage.com/post/a-simple-solution-to-a-kaggle-competition-using-xgboost-and-recipes/</a></p>
<p>I honestly havent done that much reading, but here is why I have adopted <code>recipes</code>. :<br />
* create dummies for xgboost in 1 line of code,<br />
* trainn knn models to impute all missing predictors in 1 line of code,<br />
* combine super rare categories into “other” in 1 line of code, (not done here, but all is needed is step_knnimpute(all_predictors()))</p>
<p>All while making sure that you don’t leak any test data into your train data. What’s not to like?</p>
<pre class="r"><code>

data(dataCar)

# claimcst0 = claim amount (0 if no claim)
# clm = 0 or 1 = has a claim yes/ no  
#  numclaims = number of claims  0 , 1 ,2 ,3 or 4).       
# we use clm because the corresponding dollar amount is for all claims combined.  
mydb &lt;- dataCar %&gt;% select(clm, claimcst0, exposure, veh_value, veh_body,
                           veh_age, gender, area, agecat) %&gt;% 
  mutate(annual_loss = claimcst0 / exposure)

set.seed(42)

split &lt;- rsample::initial_split(mydb, prop = 0.8)
train &lt;- rsample::training(split)
test &lt;- rsample::testing(split)

rec &lt;-  recipes::recipe(train %&gt;% select(annual_loss, clm, claimcst0,  exposure, veh_value,veh_body, veh_age, gender,area,agecat)) %&gt;% 
  recipes::update_role(everything(), new_role = &quot;predictor&quot;) %&gt;%
  recipes::update_role(annual_loss, new_role = &quot;outcome&quot;) %&gt;%
  recipes::update_role(clm, new_role = &quot;outcome&quot;) %&gt;%
  recipes::update_role(claimcst0, new_role = &quot;outcome&quot;) %&gt;%
  recipes::update_role(exposure, new_role = &quot;case weight&quot;) %&gt;%
  recipes::step_zv(recipes::all_predictors()) %&gt;%   # remove variable with all equal values
  recipes::step_other(recipes::all_predictors(), threshold = 0.01)  %&gt;%       # combine categories with less than 1% of observation
  recipes::step_dummy(recipes::all_nominal())  %&gt;% # convert to dummy for xgboost use
  check_missing(all_predictors()) ## break the bake function if any of the checked columns contains NA value

# Prepare the recipe and use juice/bake to get the data!
prepped_rec &lt;- prep(rec)
train &lt;- juice(prepped_rec)
test &lt;- bake(prepped_rec, new_data = test)</code></pre>
</div>
<div id="glms" class="section level1">
<h1>GLMs</h1>
<div id="tweedie-model-pure-premium" class="section level2">
<h2>Tweedie model (pure premium)</h2>
<p>We convert the dollar amount into an annual premium by dividing the dollar amount (claimst0) by the number of years of exposure (exposure).<br />
We weight each observation by the number of years of exposure.</p>
<p>The model isnt very impressive, with only agecat and the itnercept having a nice p-value..</p>
<pre class="r"><code>tweedie_fit &lt;- 
  glm(annual_loss ~ . -exposure -clm -claimcst0,
      family=tweedie(var.power=1.1, link.power=0),
      weights = exposure,
      data = train)

summary(tweedie_fit)
## 
## Call:
## glm(formula = annual_loss ~ . - exposure - clm - claimcst0, family = tweedie(var.power = 1.1, 
##     link.power = 0), data = train, weights = exposure)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -30.73  -15.27  -11.69   -7.48  522.10  
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      6.93764    0.93968   7.383 1.57e-13 ***
## veh_age         -0.03101    0.06872  -0.451  0.65178    
## agecat          -0.14893    0.05180  -2.875  0.00404 ** 
## veh_value_other -0.16847    0.73587  -0.229  0.81891    
## veh_body_HBACK  -0.64085    0.54507  -1.176  0.23972    
## veh_body_HDTOP  -0.48669    0.66056  -0.737  0.46126    
## veh_body_MIBUS  -0.28068    0.80548  -0.348  0.72749    
## veh_body_PANVN  -0.50868    0.77672  -0.655  0.51253    
## veh_body_SEDAN  -0.73611    0.54332  -1.355  0.17548    
## veh_body_STNWG  -0.66902    0.54715  -1.223  0.22144    
## veh_body_TRUCK  -0.66588    0.67257  -0.990  0.32215    
## veh_body_UTE    -1.02284    0.61274  -1.669  0.09507 .  
## veh_body_other  -0.63826    1.21988  -0.523  0.60082    
## gender_M         0.17536    0.14910   1.176  0.23956    
## area_B           0.03934    0.22307   0.176  0.86000    
## area_C           0.11221    0.19871   0.565  0.57228    
## area_D          -0.06155    0.27269  -0.226  0.82142    
## area_E           0.17425    0.28427   0.613  0.53990    
## area_F           0.50916    0.29573   1.722  0.08513 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Tweedie family taken to be 21321.71)
## 
##     Null deviance: 27467208  on 54284  degrees of freedom
## Residual deviance: 27072876  on 54266  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 9</code></pre>
</div>
<div id="poisson-model-frequency" class="section level2">
<h2>Poisson model (frequency)</h2>
<p>We model the presence of a claim (clm) and use the log(exposure) as an offset.</p>
<pre class="r"><code>
poisson_fit &lt;-
  glm(clm ~ . -annual_loss -exposure -claimcst0 ,
      family = poisson(link = &quot;log&quot;),
      offset = log(exposure),
      data = train)

#broom::tidy(poisson_fit)
summary(poisson_fit)
## 
## Call:
## glm(formula = clm ~ . - annual_loss - exposure - claimcst0, family = poisson(link = &quot;log&quot;), 
##     data = train, offset = log(exposure))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.7845  -0.4357  -0.3342  -0.2141   3.7458  
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -1.17070    0.24103  -4.857 1.19e-06 ***
## veh_age         -0.06614    0.01583  -4.179 2.93e-05 ***
## agecat          -0.08207    0.01190  -6.894 5.42e-12 ***
## veh_value_other  0.05618    0.19015   0.295  0.76764    
## veh_body_HBACK  -0.42944    0.14042  -3.058  0.00223 ** 
## veh_body_HDTOP  -0.29780    0.16945  -1.757  0.07884 .  
## veh_body_MIBUS  -0.35012    0.21531  -1.626  0.10393    
## veh_body_PANVN  -0.22424    0.19193  -1.168  0.24267    
## veh_body_SEDAN  -0.39995    0.13973  -2.862  0.00420 ** 
## veh_body_STNWG  -0.34514    0.14075  -2.452  0.01420 *  
## veh_body_TRUCK  -0.42734    0.17255  -2.477  0.01326 *  
## veh_body_UTE    -0.61040    0.15493  -3.940 8.16e-05 ***
## veh_body_other   0.17814    0.24225   0.735  0.46212    
## gender_M        -0.01338    0.03479  -0.385  0.70051    
## area_B           0.09313    0.04979   1.870  0.06143 .  
## area_C           0.06341    0.04542   1.396  0.16270    
## area_D          -0.07332    0.06175  -1.187  0.23508    
## area_E          -0.03182    0.06849  -0.465  0.64220    
## area_F           0.08944    0.07681   1.164  0.24423    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 18959  on 54284  degrees of freedom
## Residual deviance: 18850  on 54266  degrees of freedom
## AIC: 26206
## 
## Number of Fisher Scoring iterations: 6</code></pre>
</div>
<div id="gamma-model" class="section level2">
<h2>Gamma model</h2>
<p>For the 7% of policies with a claim, we model the dollar amount of claims (claimcst0)</p>
<pre class="r"><code>gamma_fit &lt;-
  glm(claimcst0 ~ . -annual_loss -exposure -clm ,
      data = train %&gt;% filter( claimcst0 &gt; 0),
      family = Gamma(&quot;log&quot;))

#broom::tidy(gamma_fit) 
summary(gamma_fit)
## 
## Call:
## glm(formula = claimcst0 ~ . - annual_loss - exposure - clm, family = Gamma(&quot;log&quot;), 
##     data = train %&gt;% filter(claimcst0 &gt; 0))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9859  -1.3684  -0.8208   0.0511   6.8822  
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      8.00288    0.42385  18.882  &lt; 2e-16 ***
## veh_age          0.04901    0.02809   1.744  0.08116 .  
## agecat          -0.06398    0.02088  -3.064  0.00220 ** 
## veh_value_other -0.18949    0.33484  -0.566  0.57148    
## veh_body_HBACK  -0.17503    0.24754  -0.707  0.47956    
## veh_body_HDTOP  -0.20753    0.29798  -0.696  0.48619    
## veh_body_MIBUS   0.08783    0.37897   0.232  0.81675    
## veh_body_PANVN  -0.25758    0.33745  -0.763  0.44533    
## veh_body_SEDAN  -0.30662    0.24656  -1.244  0.21374    
## veh_body_STNWG  -0.31731    0.24821  -1.278  0.20119    
## veh_body_TRUCK  -0.19878    0.30433  -0.653  0.51367    
## veh_body_UTE    -0.39258    0.27305  -1.438  0.15059    
## veh_body_other  -0.81729    0.42653  -1.916  0.05543 .  
## gender_M         0.19268    0.06126   3.146  0.00167 ** 
## area_B          -0.06782    0.08761  -0.774  0.43894    
## area_C           0.05637    0.08006   0.704  0.48141    
## area_D           0.01732    0.10891   0.159  0.87363    
## area_E           0.18760    0.12097   1.551  0.12106    
## area_F           0.43326    0.13583   3.190  0.00144 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Gamma family taken to be 3.093011)
## 
##     Null deviance: 5907.8  on 3658  degrees of freedom
## Residual deviance: 5754.2  on 3640  degrees of freedom
## AIC: 62795
## 
## Number of Fisher Scoring iterations: 7</code></pre>
</div>
</div>
<div id="xgboost" class="section level1">
<h1>XGBoost</h1>
<div id="xgboost-tweedie-model" class="section level2">
<h2>XGBoost Tweedie Model</h2>
<pre class="r"><code>xgtrain_tweedie &lt;- xgb.DMatrix(as.matrix(train %&gt;% select(-annual_loss ,-clm, - claimcst0, -exposure )),  
                               label = train$annual_loss,
                               weight = train$exposure
)


xgtest_tweedie &lt;- xgb.DMatrix(as.matrix(test %&gt;% select(-annual_loss ,-clm, - claimcst0, -exposure )),  
                              label = test$annual_loss,
                              weight = test$exposure
)



params &lt;-list(
  booster = &quot;gbtree&quot;,
  objective = &#39;reg:tweedie&#39;,
  eval_metric = &quot;tweedie-nloglik@1.1&quot;,
  tweedie_variance_power = 1.1,
  gamma = 0,
  max_depth = 4,
  eta = 0.01,
  min_child_weight = 3,
  subsample = 0.8,
  colsample_bytree = 0.8,
  tree_method = &quot;hist&quot;
)

xgcv_tweedie &lt;- xgb.cv(
  params = params,
  data = xgtrain_tweedie,
  nround = 1000,
  nfold=  5,
  showsd = TRUE,
  early_stopping_rounds = 50,
  verbose = 0)

xgcv_tweedie$best_iteration
## [1] 324


# xgb.cv doesnt ouput any model -- we need a model to predict test dataset
xgmodel_tweedie &lt;- xgboost::xgb.train(
  data = xgtrain_tweedie,
  params = params,
  nrounds = xgcv_tweedie$best_iteration,
  nthread = parallel::detectCores() - 1
)</code></pre>
</div>
<div id="xgboost-poisson-model" class="section level2">
<h2>XGBoost Poisson model</h2>
<pre class="r"><code>xgtrain_poisson &lt;- xgb.DMatrix(as.matrix(train %&gt;% select(-annual_loss ,-clm, - claimcst0, -exposure )),  
                               label = train$clm
)


xgtest_poisson &lt;- xgb.DMatrix(as.matrix(test %&gt;% select(-annual_loss ,-clm, - claimcst0, -exposure )),  
                              label = test$clm
)

setinfo(xgtrain_poisson,&quot;base_margin&quot;, 
        train %&gt;% pull(exposure) %&gt;% log() )
## [1] TRUE

setinfo(xgtest_poisson,&quot;base_margin&quot;, 
        train %&gt;% pull(exposure) %&gt;% log() )
## [1] TRUE

params &lt;-list(
  booster = &quot;gbtree&quot;,
  objective = &#39;count:poisson&#39;, 
  eval_metric = &quot;poisson-nloglik&quot;,
  gamma = 0,
  max_depth = 4,
  eta = 0.05,
  min_child_weight = 3,
  subsample = 0.8,
  colsample_bytree = 0.8,
  tree_method = &quot;hist&quot;
)

xgcv_poisson &lt;- xgb.cv(
  params = params,
  data = xgtrain_poisson,
  nround = 1000,
  nfold=  5,
  showsd = TRUE,
  early_stopping_rounds = 50,
  verbose = 0)

xgcv_poisson$best_iteration
## [1] 204


# xgb.cv doesnt ouput any model -- we need a model to predict test dataset
xgmodel_poisson &lt;- xgboost::xgb.train(
  data = xgtrain_poisson,
  params = params,
  nrounds = xgcv_poisson$best_iteration,
  nthread = parallel::detectCores() - 1
)</code></pre>
</div>
<div id="xgboost-gamma-model" class="section level2">
<h2>XGBoost Gamma model</h2>
<p>Gamma model is only train on policy with claims</p>
<pre class="r"><code>xgtrain_gamma &lt;- xgb.DMatrix(as.matrix(train %&gt;% filter(claimcst0 &gt; 0) %&gt;%select(-annual_loss ,-clm, - claimcst0, -exposure )),  
                             label = train %&gt;% filter(claimcst0 &gt; 0) %&gt;% pull(claimcst0)
)

xgtest_gamma &lt;- xgb.DMatrix(as.matrix(test %&gt;% select(-annual_loss ,-clm, - claimcst0, -exposure )),  
                            label = test$claimcst0
)

params &lt;-list(
  booster = &quot;gbtree&quot;,
  objective = &#39;reg:gamma&#39;,
  gamma = 0,
  max_depth = 4,
  eta = 0.05,
  min_child_weight = 3,
  subsample = 0.8,
  colsample_bytree = 0.8,
  tree_method = &quot;hist&quot;
)

xgcv_gamma &lt;- xgb.cv(
  params = params,
  data = xgtrain_gamma,
  nround = 1000,
  nfold=  5,
  showsd = TRUE,
  early_stopping_rounds = 50,
  verbose = 0)

xgcv_gamma$best_iteration
## [1] 239


# xgb.cv doesnt ouput any model -- we need a model to predict test dataset
xgmodel_gamma &lt;- xgboost::xgb.train(
  data = xgtrain_gamma,
  params = params,
  nrounds = xgcv_gamma$best_iteration,
  nthread = parallel::detectCores() - 1
)</code></pre>
</div>
</div>
<div id="functions-lift-double-lift" class="section level1">
<h1>Functions lift &amp; double lift</h1>
<pre class="r"><code># double lift charts
#&#39; @title disloc()
#&#39;
#&#39; @description Cette fonction crée un tableau et une double lift chart
#&#39; @param data data.frame  source
#&#39; @param pred1 prediction of first model
#&#39; @param pred1 prediction of second model
#&#39; @param expo exposure var
#&#39; @param obs observed result
#&#39; @param nb nombre de quantils créés
#&#39; @param obs_lab Label pour la valeur observée dans le graphique
#&#39; @param pred1_lab Label pour la première prédiction dans le graphique
#&#39; @param pred2_lab Label pour la deuxième prédiction dans le graphique
#&#39; @param x_label Label pour la valeur réalisée dans le graphique
#&#39; @param y_label Label pour la valeur réalisée dans le graphique
#&#39; @param y_format Fonction utilisée pour formater l&#39;axe des y dans le graphique (par exemple percent_format() ou dollar_format() du package scales)
#&#39; @export

disloc &lt;- function(data, pred1, pred2, expo, obs, nb = 10,
                   obs_lab = &quot;&quot;,
                   pred1_lab = &quot;&quot;, pred2_lab = &quot;&quot;,
                   x_label = &quot;&quot;,
                   y_label= &quot;sinistralité&quot;,
                   y_format = scales::number_format(accuracy = 1,  big.mark = &quot; &quot;, decimal.mark = &quot;,&quot;)
) {
  # obligé de mettre les variables dans un enquo pour pouvoir les utiliser dans dplyr
  
  pred1_var &lt;- enquo(pred1)
  pred2_var &lt;- enquo(pred2)
  expo_var &lt;- enquo(expo)
  obs_var &lt;- enquo(obs)
  
  
  pred1_name &lt;- quo_name(pred1_var)
  pred2_name &lt;- quo_name(pred2_var)
  obs_name &lt;- quo_name(obs_var)
  
  
  if (pred1_lab ==&quot;&quot;) {pred1_lab &lt;- pred1_name}
  if (pred2_lab ==&quot;&quot;) {pred2_lab &lt;- pred2_name}
  if (obs_lab ==&quot;&quot;) {obs_lab &lt;- obs_name}
  
  if (x_label == &quot;&quot;){ x_label &lt;- paste0(&quot;ratio entre les prédictions &quot;, pred1_lab, &quot; / &quot;, pred2_lab)}
  
  # création de la comparaison entre les deux pred
  dd &lt;- data %&gt;%
    mutate(ratio = !!pred1_var / !!pred2_var) %&gt;%
    filter(!!expo_var &gt; 0) %&gt;%
    drop_na()
  
  # constitution des buckets de poids égaux
  dd &lt;- dd %&gt;% add_equal_weight_group(
    sort_by = ratio,
    expo = !!expo_var, 
    group_variable_name = &quot;groupe&quot;,
    nb = nb
  )
  
  # comparaison sur ces buckets
  dd &lt;- full_join(
    dd %&gt;% group_by(groupe) %&gt;%
      summarise(
        ratio_moyen = mean(ratio),
        ratio_min = min(ratio),
        ratio_max = max(ratio)
      ),
    dd %&gt;% group_by(groupe) %&gt;%
      summarise_at(
        funs(sum(.) / sum(!!expo_var)),
        .vars = vars(!!obs_var, !!pred1_var, !!pred2_var)
      ) %&gt;%
      ungroup,
    by = &quot;groupe&quot;
  )
  
  # création des labels
  dd &lt;- dd %&gt;%
    mutate(labs = paste0(&quot;[&quot;, round(ratio_min, 2), &quot;, &quot;, round(ratio_max, 2), &quot;]&quot;))
  
  # graphe
  plotdata &lt;-
    dd %&gt;%
    gather(key, variable, !!obs_var, !!pred1_var, !!pred2_var) %&gt;%
    ## Pas optimal mais je ne trouve pas mieux...
    mutate(key = case_when(
      key == obs_name ~ obs_lab,
      key == pred1_name ~ pred1_lab,
      key == pred2_name ~ pred2_lab
    )) %&gt;%
    mutate(key = factor(key, levels = c(obs_lab, pred1_lab, pred2_lab), ordered = TRUE))
  
  pl &lt;- plotdata %&gt;%
    ggplot(aes(ratio_moyen, variable, color = key, linetype = key)) +
    cowplot::theme_cowplot() +
    cowplot::background_grid()+
    colorblindr::scale_color_OkabeIto( ) + 
    
    scale_x_continuous(breaks = scales::pretty_breaks())+
    geom_line() +
    geom_point() +
    scale_x_continuous(breaks = dd$ratio_moyen, labels = dd$labs) +
    scale_y_continuous(breaks = scales::pretty_breaks() )+  
    labs(
      x = x_label,
      y = y_label
    )+
    theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)) #+
  
  
  # écart au réalisé, pondéré
  ecart &lt;- dd %&gt;%
    mutate(poids = abs(1 - ratio_moyen)) %&gt;%
    summarise_at(
      vars(!!pred1_var, !!pred2_var),
      funs(weighted.mean((. - !!obs_var)^2, w = poids) %&gt;% sqrt())
    ) %&gt;% summarise(ratio_distance = !!pred2_var / !!pred1_var) %&gt;%
    as.numeric()
  
  list(
    graphe = pl,
    ecart = ecart,
    tableau = dd
  )
}

#&#39; @title add_equal_weight_group()
#&#39;
#&#39; @description Cette fonction crée des groupe (quantiles) avec le nombre nombre total d&#39;exposition.
#&#39; @param table data.frame  source
#&#39; @param sort_by Variable utilisée pour trier les observations.
#&#39; @param expo Exposition (utilisée pour créer des quantiles de la même taille.  Si NULL, l&#39;exposition est égale pour toutes les observations) (Défault = NULL).
#&#39; @param nb Nombre de quantiles crées (défaut = 10)
#&#39; @param group_variable_name Nom de la variable de groupes créée
#&#39; @export


add_equal_weight_group &lt;- function(table, sort_by, expo = NULL, group_variable_name = &quot;groupe&quot;, nb = 10) {
  sort_by_var &lt;- enquo(sort_by)
  groupe_variable_name_var &lt;- enquo(group_variable_name)
  
  if (!(missing(expo))){ # https://stackoverflow.com/questions/48504942/testing-a-function-that-uses-enquo-for-a-null-parameter
    
    expo_var &lt;- enquo(expo)
    
    total &lt;- table %&gt;% pull(!!expo_var) %&gt;% sum
    br &lt;- seq(0, total, length.out = nb + 1) %&gt;% head(-1) %&gt;% c(Inf) %&gt;% unique
    table %&gt;%
      arrange(!!sort_by_var) %&gt;%
      mutate(cumExpo = cumsum(!!expo_var)) %&gt;%
      mutate(!!group_variable_name := cut(cumExpo, breaks = br, ordered_result = TRUE, include.lowest = TRUE) %&gt;% as.numeric) %&gt;%
      select(-cumExpo)
  } else {
    total &lt;- nrow(table)
    br &lt;- seq(0, total, length.out = nb + 1) %&gt;% head(-1) %&gt;% c(Inf) %&gt;% unique
    table %&gt;%
      arrange(!!sort_by_var) %&gt;%
      mutate(cumExpo = row_number()) %&gt;%
      mutate(!!group_variable_name := cut(cumExpo, breaks = br, ordered_result = TRUE, include.lowest = TRUE) %&gt;% as.numeric) %&gt;%
      select(-cumExpo)
  }
}

get_lift_chart_data &lt;- function(
  data, 
  sort_by,
  pred, 
  expo, 
  obs, 
  nb = 10) {
  
  pred_var &lt;- enquo(pred)
  sort_by_var &lt;- enquo(sort_by)
  expo_var &lt;- enquo(expo)
  obs_var &lt;- enquo(obs)
  
  
  pred_name &lt;- quo_name(pred_var)
  sort_by_name &lt;- quo_name(sort_by_var)
  obs_name &lt;- quo_name(obs_var)
  
  # constitution des buckets de poids égaux
  dd &lt;- data %&gt;% add_equal_weight_group(
    sort_by = !!sort_by_var,
    expo = !!expo_var, 
    group_variable_name = &quot;groupe&quot;,
    nb = nb
  )
  
  # comparaison sur ces buckets
  dd &lt;- full_join(
    dd %&gt;% 
      group_by(groupe) %&gt;%
      summarise(
        exposure = sum(!!expo_var),
        sort_by_moyen = mean(!!sort_by_var),
        sort_by_min = min(!!sort_by_var),
        sort_by_max = max(!!sort_by_var)
      ) %&gt;%
      ungroup(),
    dd %&gt;% 
      group_by(groupe) %&gt;%
      summarise_at(
        funs(sum(.) / sum(!!expo_var)),
        .vars = vars(!!obs_var, !!pred_var)
      ) %&gt;%
      ungroup,
    by = &quot;groupe&quot;
  )
  
  # création des labels
  dd &lt;- dd %&gt;%
    mutate(labs = paste0(&quot;[&quot;, round(sort_by_min, 2), &quot;, &quot;, round(sort_by_max, 2), &quot;]&quot;))
  
}

get_lift_chart &lt;- function(data, 
                           sort_by,
                           pred, 
                           expo, 
                           obs, 
                           nb){
  
  pred_var &lt;- enquo(pred)
  sort_by_var &lt;- enquo(sort_by)
  expo_var &lt;- enquo(expo)
  obs_var &lt;- enquo(obs)
  
  lift_data &lt;- get_lift_chart_data(
    data = data, 
    sort_by = !!sort_by_var,
    pred = !!pred_var, 
    expo = !!expo_var, 
    obs = !!obs_var, 
    nb = 10)
  
  p1 &lt;- lift_data %&gt;% 
    mutate(groupe = as.factor(groupe)) %&gt;%
    select(groupe, labs, !!pred_var, !!obs_var) %&gt;%
    gather(key = type, value = average, !!pred_var, !!obs_var)  %&gt;% 
    ggplot(aes(x= groupe, y = average, color =type, group = type)) +
    geom_line() +
    geom_point()+
    cowplot::theme_half_open() +
    cowplot::background_grid() +
    colorblindr::scale_color_OkabeIto( ) + 
    # scale_y_continuous(
    #   breaks  = scales::pretty_breaks()
    # )+
    theme(
      legend.position = c(0.1, 0.8),
      axis.title.x=element_blank(),
      axis.text.x=element_blank(),
      axis.ticks.x=element_blank()
    )
  
  p2 &lt;- lift_data %&gt;% 
    mutate(groupe = as.factor(groupe)) %&gt;%
    select(groupe, labs, exposure) %&gt;%
    ggplot(aes(x= groupe, y = exposure)) +
    geom_col() +
    cowplot::theme_half_open() +
    cowplot::background_grid() +
    colorblindr::scale_color_OkabeIto( )# + 
    # scale_y_continuous(
    #   expand = c(0,0),
    #   breaks  = scales::breaks_pretty(3)
    # )
  
  return(p1 / p2 +   plot_layout(heights = c(3, 1)))
}</code></pre>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<p>Add predictions to the test dataset.<br />
Quick check: the means for predictions of frequency and total cost don’t appear to be way off</p>
<pre class="r"><code>test_w_preds &lt;- test %&gt;%
  add_predictions(&quot;pred_annual_loss_glm&quot;, model = tweedie_fit, type = &quot;response&quot;) %&gt;%
  mutate(pred_loss_glm = pred_annual_loss_glm * exposure) %&gt;%
  add_predictions(&quot;pred_poisson_glm&quot;, model = poisson_fit, type = &quot;response&quot;) %&gt;%
  add_predictions(&quot;pred_gamma_glm&quot;, model = gamma_fit, type = &quot;response&quot;)  %&gt;%
  mutate(pred_frequency_severity_glm = pred_poisson_glm * pred_gamma_glm,
         pred_annual_loss_xgboost = predict(xgmodel_tweedie, xgtest_tweedie),
         pred_loss_xgboost = pred_annual_loss_xgboost * exposure,
         pred_poisson_xgboost =  predict(xgmodel_poisson, xgtest_poisson),
         pred_gamma_xgboost =  predict(xgmodel_gamma, xgtest_gamma),
         pred_frequency_severity_xgboost = pred_poisson_xgboost * pred_gamma_xgboost,
         pred_annual_freq_glm = pred_poisson_glm / exposure,
         pred_annual_freq_xgboost = pred_poisson_xgboost / exposure,)


test_w_preds %&gt;%
  select(claimcst0, pred_loss_glm, pred_frequency_severity_glm, pred_loss_xgboost, pred_frequency_severity_xgboost,
         clm, pred_poisson_glm, pred_poisson_xgboost) %&gt;% 
  skimr::skim() %&gt;%
  knitr::kable(digits = 3L)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">skim_type</th>
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">numeric.mean</th>
<th align="right">numeric.sd</th>
<th align="right">numeric.p0</th>
<th align="right">numeric.p25</th>
<th align="right">numeric.p50</th>
<th align="right">numeric.p75</th>
<th align="right">numeric.p100</th>
<th align="left">numeric.hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">claimcst0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">140.151</td>
<td align="right">957.834</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">23722.430</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">pred_loss_glm</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">136.031</td>
<td align="right">97.982</td>
<td align="right">0.413</td>
<td align="right">58.817</td>
<td align="right">120.776</td>
<td align="right">194.181</td>
<td align="right">705.105</td>
<td align="left">▇▅▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">pred_frequency_severity_glm</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">136.036</td>
<td align="right">97.671</td>
<td align="right">0.417</td>
<td align="right">58.876</td>
<td align="right">120.862</td>
<td align="right">194.512</td>
<td align="right">704.338</td>
<td align="left">▇▅▁▁▁</td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">pred_loss_xgboost</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">125.613</td>
<td align="right">122.959</td>
<td align="right">0.352</td>
<td align="right">53.090</td>
<td align="right">108.917</td>
<td align="right">177.029</td>
<td align="right">6830.249</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">pred_frequency_severity_xgboost</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">140.075</td>
<td align="right">43.468</td>
<td align="right">48.720</td>
<td align="right">114.607</td>
<td align="right">138.654</td>
<td align="right">156.335</td>
<td align="right">945.439</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">clm</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.071</td>
<td align="right">0.257</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">1.000</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">pred_poisson_glm</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.067</td>
<td align="right">0.044</td>
<td align="right">0.000</td>
<td align="right">0.031</td>
<td align="right">0.062</td>
<td align="right">0.100</td>
<td align="right">0.334</td>
<td align="left">▇▆▁▁▁</td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">pred_poisson_xgboost</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.075</td>
<td align="right">0.013</td>
<td align="right">0.040</td>
<td align="right">0.067</td>
<td align="right">0.075</td>
<td align="right">0.082</td>
<td align="right">0.188</td>
<td align="left">▃▇▁▁▁</td>
</tr>
</tbody>
</table>
<div id="normalized-gini" class="section level2">
<h2>Normalized Gini</h2>
<p>The GLM model systematically perform better than the XGB models.<br />
The XGB combination of Frequency and Severity models has a Normalized Gini 4x smaller than it’s GLM counterpart.</p>
<p>We will really need to do this exercise again with a larger dataset.</p>
<pre class="r"><code>Poisson_GLM = MLmetrics::NormalizedGini(test_w_preds$clm, test_w_preds$pred_poisson_glm)
Poisson_XGB = MLmetrics::NormalizedGini(test_w_preds$clm, test_w_preds$pred_poisson_xgboost)
Tweedie_GLM = MLmetrics::NormalizedGini(test_w_preds$claimcst0, test_w_preds$pred_loss_glm)
Tweedie_XGB = MLmetrics::NormalizedGini(test_w_preds$claimcst0, test_w_preds$pred_loss_xgboost)
FreqSev_GLM = MLmetrics::NormalizedGini(test_w_preds$claimcst0, test_w_preds$pred_frequency_severity_glm)
FreqSev_XGB = MLmetrics::NormalizedGini(test_w_preds$claimcst0, test_w_preds$pred_frequency_severity_xgboost    )


tibble(Poisson_GLM,
       Poisson_XGB,
       Tweedie_GLM,
       Tweedie_XGB,
       FreqSev_GLM,
       FreqSev_XGB) %&gt;%
  gather(key = model, value = NormalizedGini) %&gt;%
  knitr::kable(digits = 3L)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">NormalizedGini</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Poisson_GLM</td>
<td align="right">0.028</td>
</tr>
<tr class="even">
<td align="left">Poisson_XGB</td>
<td align="right">0.011</td>
</tr>
<tr class="odd">
<td align="left">Tweedie_GLM</td>
<td align="right">0.032</td>
</tr>
<tr class="even">
<td align="left">Tweedie_XGB</td>
<td align="right">0.038</td>
</tr>
<tr class="odd">
<td align="left">FreqSev_GLM</td>
<td align="right">0.032</td>
</tr>
<tr class="even">
<td align="left">FreqSev_XGB</td>
<td align="right">0.008</td>
</tr>
</tbody>
</table>
</div>
<div id="lift-charts-of-select-models" class="section level2">
<h2>Lift charts of select models</h2>
<div id="tweedie-glm-pure-premium" class="section level3">
<h3>Tweedie GLM (pure premium)</h3>
<pre class="r"><code>get_lift_chart(
  data = test_w_preds,
  sort_by= pred_annual_loss_glm, 
  pred = pred_loss_glm, 
  obs = claimcst0, 
  expo  = exposure )</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-11-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="tweedie-xgboost-pure-premium" class="section level3">
<h3>Tweedie XGBoost (pure premium)</h3>
<pre class="r"><code>
get_lift_chart(
  data = test_w_preds,
  sort_by= pred_annual_loss_xgboost, 
  pred = pred_loss_xgboost, 
  obs = claimcst0, 
  expo  = exposure )</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-12-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="poisson-glm-frequency" class="section level3">
<h3>Poisson GLM (frequency)</h3>
<pre class="r"><code>get_lift_chart(
  data = test_w_preds, 
  sort_by= pred_annual_freq_glm, 
  pred = pred_poisson_glm,
  obs = clm, 
  expo  = exposure )</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-13-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="poisson-xgboost-frequency" class="section level3">
<h3>Poisson XGBoost (frequency)</h3>
<pre class="r"><code>get_lift_chart(
  data = test_w_preds, 
  sort_by= pred_annual_freq_xgboost, 
  pred = pred_poisson_xgboost,
  obs = clm, 
  expo  = exposure )</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-14-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="a-test-extract-individual-plot-from-a-patchwork-object-it-works" class="section level3">
<h3>A test: extract individual plot from a patchwork object (it works!)</h3>
<pre class="r"><code>gaa2 &lt;- get_lift_chart(
  data = test_w_preds, 
  sort_by= pred_annual_freq_glm, 
  pred = pred_poisson_glm,
  obs = clm, 
  expo  = exposure )

gaa1 &lt;- get_lift_chart(
  data = test_w_preds, 
  sort_by= pred_annual_freq_xgboost, 
  pred = pred_poisson_xgboost,
  obs = clm, 
  expo  = exposure )

(gaa1[[1]] + gaa2[[1]]) /(gaa1[[2]] + gaa2[[2]])  +    plot_layout(heights = c(3, 1))</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-15-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="double-lift-chart" class="section level2">
<h2>Double lift chart</h2>
<div id="glm-tweedie-vs-glm-poisson-gamma" class="section level3">
<h3>GLM Tweedie vs GLM Poisson * Gamma</h3>
<p>GLM Tweedie ( power = 1.1) appears similar to than Poisson * Gamma .</p>
<pre class="r"><code>disloc(data = test_w_preds, 
       pred1 = pred_loss_glm, 
       pred2 = pred_frequency_severity_glm, 
       expo = exposure, 
       obs = claimcst0 ,
       y_label = &quot;coût moyen ($)&quot;
) %&gt;% .$graphe</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-16-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="glm-tweedie-vs-xgb-tweedie" class="section level3">
<h3>GLM Tweedie vs XGB Tweedie</h3>
<p>Why is the xgboost tweedie doing so poorly vs GLM tweedie?<br />
partial answer: xgboost is out of fold, glm can try to overfit.. but it really doesnt have that many variables to overfit on.</p>
<pre class="r"><code>
disloc(data = test_w_preds, 
       pred1 = pred_loss_xgboost , 
       pred2 = pred_loss_glm, 
       expo = exposure, 
       obs = claimcst0 ,
       y_label = &quot;coût moyen ($)&quot;
) %&gt;% .$graphe</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-17-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="glm-poissongamma-vs-xgb-poissongamma" class="section level3">
<h3>GLM Poisson<em>Gamma vs XGB Poisson</em>Gamma</h3>
<pre class="r"><code>disloc(data = test_w_preds, 
       pred1 = pred_frequency_severity_glm, 
       pred2 = pred_frequency_severity_xgboost, 
       expo = exposure, 
       obs = claimcst0 ,
       y_label = &quot;coût moyen ($)&quot;
) %&gt;% .$graphe</code></pre>
<p><img src="/post/2020-03-23-tweedie-vs-poisson-gamma_files/figure-html/unnamed-chunk-18-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
