---
title: "Bayesian optimization of xgboost hyperparameters for a Poisson regression in R"
author: "Simon Coulombe"
date: 2019-01-09
slug: "bayesian"
output:
  blogdown::html_page:
    toc: false
categories: ["R"]
tags: ["bayesian", "optimization", "rstats", "mlrMBO", "caret", "mlr" ,"poisson", "rBayesianOptimization"]
---



<p>UPDATE 2020: skimr v2 now produces nice html in rmarkdown, so skimr::kable() has been deprecated. <a href="https://www.r-bloggers.com/reintroducing-skimr-v2-a-year-in-the-life-of-an-open-source-r-project/" class="uri">https://www.r-bloggers.com/reintroducing-skimr-v2-a-year-in-the-life-of-an-open-source-r-project/</a></p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Ratemaking models in insurance routinely use Poisson regression to model the frequency of auto insurance claims. They usually are GLMs but some insurers are moving towards GBMs, such as <code>xgboost</code>.</p>
<p><code>xgboost</code>has multiple hyperparameters that can be tuned to obtain a better predictive power. There are multiple ways to tune these hyperparameters. In order of efficiency are the grid search, the random search and the bayesian optimization search.</p>
<p>In this post, we will compare the results of xgboost hyperparameters for a Poisson regression in R using a random search versus a bayesian search. Two packages wills be compared for the bayesian approach: the <code>mlrMBO</code> package and the <code>rBayesianOptimization</code> package.</p>
<p>We will model the number of auto insurance claims based on characteristics of the car and driver, while offsetting for exposure. The data comes from the <code>insuranceData</code> package.</p>
<p>For most model types, <code>mlrMBO</code> can be used in combination with the <code>mlr</code> package to find the best hyperparameters directly. As far as I know the <code>mlr</code> package doesnt <a href="https://github.com/mlr-org/mlr/issues/515">handle poisson regression</a>, so we will have to create our own function to maximise.</p>
<p>I tried the <code>rBayesianOptimization</code> package after being inspired by this post from <a href="http://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html">Max Kuhn</a> from 2016. I do not recommend using this package because it <a href="https://github.com/yanyachen/rBayesianOptimization/issues/4">sometimes recycles hyperparameters</a> and hasnt been updated on github since 2016.</p>
<p><strong>Keep your eyes peeled</strong>: Max Kuhn (<span class="citation">@topepos</span>) <a href="https://twitter.com/topepos/status/1075151561863692290">said that tidymodels might do this in the first half of 2019</a>.</p>
<pre class="r"><code>library(xgboost)
library(insuranceData) # example dataset https://cran.r-project.org/web/packages/insuranceData/insuranceData.pdf
library(tidyverse) # for data wrangling
library(rBayesianOptimization) # to create cv folds and for bayesian optimisation
library(mlrMBO)  # for bayesian optimisation  
library(skimr) # for summarising databases
library(purrr) # to evaluate the loglikelihood of each parameter set in the random grid search
require(&quot;DiceKriging&quot;) # mlrmbo requires this
require(&quot;rgenoud&quot;) # mlrmbo requires this

switch_generate_interim_data &lt;- FALSE</code></pre>
</div>
<div id="preparing-the-data" class="section level1">
<h1>Preparing the data</h1>
<p>First, we load the dataCar data from the <code>insuranceData</code> package. It contains 67 856 one-year vehicle insurance policies taken out in 2004 or 2005.</p>
<p>The dependent variable is <code>numclaims</code>, which represents the number of claims.</p>
<p>The <code>exposure</code> variable represents the “number of year of exposure” and is used as the offset variable. It is bounded between 0 and 1.</p>
<p>Finally, the independent variables are as follow:</p>
<ul>
<li><code>veh_value</code>, the vehicle value in tens of thousand of dollars,<br />
</li>
<li><code>veh_body</code>, y vehicle body, coded as BUS CONVT COUPE HBACK HDTOP MCARA MIBUS PANVN RDSTR SEDAN STNWG TRUCK UTE,<br />
</li>
<li><code>veh_age</code>, 1 (youngest), 2, 3, 4,<br />
</li>
<li><code>gender</code>, a factor with levels F M,<br />
</li>
<li><code>area</code> a factor with levels A B C D E F,<br />
</li>
<li><code>agecat</code> 1 (youngest), 2, 3, 4, 5, 6</li>
</ul>
<pre class="r"><code># load insurance data
data(dataCar)
mydb &lt;- dataCar %&gt;% select(numclaims, exposure, veh_value, veh_body,
                           veh_age, gender, area, agecat)
label_var &lt;- &quot;numclaims&quot;  
offset_var &lt;- &quot;exposure&quot;
feature_vars &lt;- mydb %&gt;% 
  select(-one_of(c(label_var, offset_var))) %&gt;% 
  colnames()

#skimr::skim(mydb ) %&gt;% 
  #skimr::kable() #
  # update 2020 : Now, calling skim(), within an RMarkdown doc should produce something
  # nice by default.
  # 
  # skim(chickwts)
  # 
  # Data summary Name   chickwts
  # Number of rows  71
  # Number of columns   2
  # _______________________     
  # Column type frequency:  
# factor    1
# numeric   1
# ________________________  
# Group variables   None
# 
# Variable type: factor
# skim_variable     n_missing   complete_rate   ordered     n_unique    top_counts
# feed  0   1   FALSE   6   soy: 14, cas: 12, lin: 12, sun: 12
# 
# Variable type: numeric
# skim_variable     n_missing   complete_rate   mean    sd  p0  p25     p50     p75     p100    hist
# weight    0   1   261.31  78.07   108     204.5   258     323.5   423     ▆▆▇▇▃
# 
# You get a nice html version of both the summary header and the skimr subtables
# for each type of data.
# 
# In this context, you configure the output the same way you handle other knitr
# code chunks.
# 
# This means that we’re dropping direct support for kable.skim_df() and
# pander.skim_df(). But you can still get pretty similar results to these</code></pre>
<p>Insurance ratemaking often requires monotonous relationships. In our case, we will arbitrarily force the number of claims to be non-increasing with the age of the vehicle.</p>
<p>The code below imports the data, one-hot encodes dummy variables, converts the data frame to a xgb.DMatrix for xgboost consumption, sets the offset for exposure, sets the constraints and defines the 3 folds we will use for cross-validation.</p>
<pre class="r"><code># one hot encoding of categorical (factor) data
myformula &lt;- paste0( &quot;~&quot;, paste0( feature_vars, collapse = &quot; + &quot;) ) %&gt;% 
  as.formula()

dummyFier &lt;- caret::dummyVars(myformula, data=mydb, fullRank = TRUE)
dummyVars.df &lt;- predict(dummyFier,newdata = mydb)
mydb_dummy &lt;- cbind(mydb %&gt;% select(one_of(c(label_var, offset_var))), 
                    dummyVars.df)
rm(myformula, dummyFier, dummyVars.df)

# get  list the column names of the db with the dummy variables
feature_vars_dummy &lt;-  mydb_dummy  %&gt;% 
  select(-one_of(c(label_var, offset_var))) %&gt;% 
  colnames()

# create xgb.matrix for xgboost consumption
mydb_xgbmatrix &lt;- xgb.DMatrix(
  data = mydb_dummy %&gt;% select(feature_vars_dummy) %&gt;% as.matrix, 
  label = mydb_dummy %&gt;% pull(label_var),
  missing = &quot;NAN&quot;)

#base_margin: apply exposure offset 
setinfo(mydb_xgbmatrix,&quot;base_margin&quot;, 
        mydb %&gt;% pull(offset_var) %&gt;% log() )</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code># a fake constraint, just to show how it is done.  
#Here we force &quot;the older the car, the less likely are claims&quot;
myConstraint   &lt;- data_frame(Variable = feature_vars_dummy) %&gt;%
  mutate(sens = ifelse(Variable == &quot;veh_age&quot;, -1, 0))

# random folds for xgb.cv
cv_folds = rBayesianOptimization::KFold(mydb_dummy$numclaims, 
                                        nfolds= 3,
                                        stratified = TRUE,
                                        seed= 0)</code></pre>
</div>
<div id="example-1-optimize-hyperparameters-using-a-random-search-non-bayesian" class="section level1">
<h1>Example 1: Optimize hyperparameters using a random search (non bayesian)</h1>
<p>We will start with a quick example of random search.</p>
<p>I don’t use caret for the random search <a href="https://github.com/topepo/caret/issues/861">because it has a hard time with poisson regression</a>.</p>
<p>First, we generate 20 random sets of hyperparameters. I will force <code>gamma = 0</code> for half the sets.I will also hardcode an extra set of parameters named <code>simon_params</code> because I find this combination is often a good starting point.</p>
<pre class="r"><code># generate hard coded parameters
simon_params &lt;- data.frame(max_depth = 6,
                           colsample_bytree= 0.8,
                           subsample = 0.8,
                           min_child_weight = 3,
                           eta  = 0.01,
                           gamma = 0,
                           nrounds = 200) %&gt;% 
  as_tibble()
# generate 20 random models
how_many_models &lt;- 20
max_depth &lt;-        data.frame(max_depth = floor(runif(how_many_models)*5 ) + 3)  # 1 à 4
colsample_bytree &lt;- data.frame(colsample_bytree =runif(how_many_models) * 0.8 + 0.2)  # 0.2 à 1
subsample &lt;-        data.frame(subsample =runif(how_many_models) * 0.8 + 0.2) # 0.2 à 1
min_child_weight &lt;- data.frame(min_child_weight = floor(runif(how_many_models) * 10) + 1) # 1 à 10
eta &lt;-              data.frame(eta = runif(how_many_models) * 0.06 + 0.002) # 0.002 à 0.062
gamma &lt;-            data.frame(gamma =c(rep(0,how_many_models/2), runif(how_many_models/2)*10)) # 0 à 10
nrounds &lt;-          data.frame(nrounds = rep(2e2,how_many_models)) # max 200

random_grid &lt;-max_depth %&gt;%
  bind_cols(colsample_bytree ) %&gt;%
  bind_cols(subsample) %&gt;%
  bind_cols(min_child_weight) %&gt;%
  bind_cols(eta) %&gt;%
  bind_cols(gamma) %&gt;%
  bind_cols(nrounds)  %&gt;% as_tibble()
# combine random and hardcoded parameters
df.params &lt;- simon_params %&gt;%  bind_rows(random_grid) %&gt;%
  mutate(rownum = row_number(),
         rownumber = row_number())
list_of_param_sets &lt;- df.params %&gt;% nest(-rownum)</code></pre>
<p>Here are the hyperparameters that will be tested:</p>
<pre class="r"><code>knitr::kable(df.params)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">max_depth</th>
<th align="right">colsample_bytree</th>
<th align="right">subsample</th>
<th align="right">min_child_weight</th>
<th align="right">eta</th>
<th align="right">gamma</th>
<th align="right">nrounds</th>
<th align="right">rownum</th>
<th align="right">rownumber</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.8000000</td>
<td align="right">0.8000000</td>
<td align="right">3</td>
<td align="right">0.0100000</td>
<td align="right">0.000000</td>
<td align="right">200</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">0.3697140</td>
<td align="right">0.7176482</td>
<td align="right">3</td>
<td align="right">0.0447509</td>
<td align="right">0.000000</td>
<td align="right">200</td>
<td align="right">2</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0.7213390</td>
<td align="right">0.8263462</td>
<td align="right">5</td>
<td align="right">0.0259997</td>
<td align="right">0.000000</td>
<td align="right">200</td>
<td align="right">3</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.3004441</td>
<td align="right">0.6424290</td>
<td align="right">4</td>
<td align="right">0.0215211</td>
<td align="right">0.000000</td>
<td align="right">200</td>
<td align="right">4</td>
<td align="right">4</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.4137765</td>
<td align="right">0.6237757</td>
<td align="right">7</td>
<td align="right">0.0474252</td>
<td align="right">0.000000</td>
<td align="right">200</td>
<td align="right">5</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.5088913</td>
<td align="right">0.8314850</td>
<td align="right">3</td>
<td align="right">0.0141615</td>
<td align="right">0.000000</td>
<td align="right">200</td>
<td align="right">6</td>
<td align="right">6</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.2107123</td>
<td align="right">0.2186650</td>
<td align="right">5</td>
<td align="right">0.0446673</td>
<td align="right">0.000000</td>
<td align="right">200</td>
<td align="right">7</td>
<td align="right">7</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.5059104</td>
<td align="right">0.5817841</td>
<td align="right">8</td>
<td align="right">0.0093015</td>
<td align="right">0.000000</td>
<td align="right">200</td>
<td align="right">8</td>
<td align="right">8</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.8957527</td>
<td align="right">0.7858510</td>
<td align="right">1</td>
<td align="right">0.0167293</td>
<td align="right">0.000000</td>
<td align="right">200</td>
<td align="right">9</td>
<td align="right">9</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.4722792</td>
<td align="right">0.7541852</td>
<td align="right">9</td>
<td align="right">0.0105983</td>
<td align="right">0.000000</td>
<td align="right">200</td>
<td align="right">10</td>
<td align="right">10</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.5856641</td>
<td align="right">0.5820957</td>
<td align="right">4</td>
<td align="right">0.0163778</td>
<td align="right">0.000000</td>
<td align="right">200</td>
<td align="right">11</td>
<td align="right">11</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.6796527</td>
<td align="right">0.8889676</td>
<td align="right">9</td>
<td align="right">0.0055361</td>
<td align="right">3.531973</td>
<td align="right">200</td>
<td align="right">12</td>
<td align="right">12</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.5948330</td>
<td align="right">0.5504777</td>
<td align="right">4</td>
<td align="right">0.0405373</td>
<td align="right">2.702602</td>
<td align="right">200</td>
<td align="right">13</td>
<td align="right">13</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">0.3489741</td>
<td align="right">0.3958378</td>
<td align="right">4</td>
<td align="right">0.0545762</td>
<td align="right">9.926841</td>
<td align="right">200</td>
<td align="right">14</td>
<td align="right">14</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.8618987</td>
<td align="right">0.2565432</td>
<td align="right">5</td>
<td align="right">0.0487349</td>
<td align="right">6.334933</td>
<td align="right">200</td>
<td align="right">15</td>
<td align="right">15</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">0.7347734</td>
<td align="right">0.2795729</td>
<td align="right">9</td>
<td align="right">0.0498385</td>
<td align="right">2.132081</td>
<td align="right">200</td>
<td align="right">16</td>
<td align="right">16</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.8353919</td>
<td align="right">0.4530174</td>
<td align="right">9</td>
<td align="right">0.0293165</td>
<td align="right">1.293724</td>
<td align="right">200</td>
<td align="right">17</td>
<td align="right">17</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.2863549</td>
<td align="right">0.6149074</td>
<td align="right">4</td>
<td align="right">0.0266050</td>
<td align="right">4.781180</td>
<td align="right">200</td>
<td align="right">18</td>
<td align="right">18</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.7789688</td>
<td align="right">0.7296041</td>
<td align="right">8</td>
<td align="right">0.0506522</td>
<td align="right">9.240745</td>
<td align="right">200</td>
<td align="right">19</td>
<td align="right">19</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.5290195</td>
<td align="right">0.5254641</td>
<td align="right">10</td>
<td align="right">0.0382960</td>
<td align="right">5.987610</td>
<td align="right">200</td>
<td align="right">20</td>
<td align="right">20</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.8567570</td>
<td align="right">0.9303007</td>
<td align="right">5</td>
<td align="right">0.0412834</td>
<td align="right">9.761707</td>
<td align="right">200</td>
<td align="right">21</td>
<td align="right">21</td>
</tr>
</tbody>
</table>
<p>Evaluate these 21 models with 3 folds and retourn the loglikelihood of each models:</p>
<pre class="r"><code>if (switch_generate_interim_data){
  
  random_grid_results &lt;- list_of_param_sets %&gt;% 
    mutate(booster = map(data, function(X){
      message(paste0(&quot;model #&quot;,       X$rownumber,
                     &quot; eta = &quot;,              X$eta,
                     &quot; max.depth = &quot;,        X$max_depth,
                     &quot; min_child_weigth = &quot;, X$min_child_weight,
                     &quot; subsample = &quot;,        X$subsample,
                     &quot; colsample_bytree = &quot;, X$colsample_bytree,
                     &quot; gamma = &quot;,            X$gamma, 
                     &quot; nrounds = &quot;,          X$nrounds))
      set.seed(1234)
      
      cv &lt;- xgb.cv(
        params = list(
          booster = &quot;gbtree&quot;,
          eta = X$eta,
          max_depth = X$max_depth,
          min_child_weight = X$min_child_weight,
          gamma = X$gamma,
          subsample = X$subsample,
          colsample_bytree = X$colsample_bytree,
          objective = &#39;count:poisson&#39;, 
          eval_metric = &quot;poisson-nloglik&quot;),
        data = mydb_xgbmatrix,
        nround = X$nrounds,
        folds=  cv_folds,
        monotone_constraints = myConstraint$sens,
        prediction = FALSE,
        showsd = TRUE,
        early_stopping_rounds = 50,
        verbose = 0)
      
      function_return &lt;- list(Score = cv$evaluation_log[, max(test_poisson_nloglik_mean)], # l&#39;itération qui a la plus haute moyenne de nloglik sur toutes les cv folds
                              Pred = 0)
      
      message(paste0(&quot;Score :&quot;, function_return$Score))
      return(function_return)})) %&gt;%
    mutate(Score =  pmap(list(booster), function(X){X$Score })%&gt;% unlist())
  
  
  write_rds(random_grid_results, here::here(&quot;content/post/data/interim/random_grid_results.rds&quot;))
} else {random_grid_results &lt;- read_rds( here::here(&quot;content/post/data/interim/random_grid_results.rds&quot;))}</code></pre>
<pre class="r"><code>random_grid_results %&gt;%
  mutate( hardcoded = ifelse(rownum ==1,TRUE,FALSE)) %&gt;%
  ggplot(aes( x = rownum, y = Score, color = hardcoded)) + 
  geom_point() +
  labs(title = &quot;random grid search&quot;)+
  ylab(&quot;loglikelihood&quot;)</code></pre>
<p><img src="/post/2019-01-09-mlrmbo_poisson_files/figure-html/unnamed-chunk-4-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="example-2-bayesian-optimization-using-mlrmbo" class="section level1">
<h1>Example #2 Bayesian optimization using <code>mlrMBO</code></h1>
<p>This tutorial builds on the <a href="https://mlrmbo.mlr-org.com/articles/mlrMBO.html">mlrMBO vignette</a></p>
<p>First, we need to define the objective function that the bayesian search will try to maximise. In this case we want to maximise the log likelihood of the out of fold predictions.</p>
<pre class="r"><code># objective function: we want to maximise the log likelihood by tuning most parameters
obj.fun  &lt;- smoof::makeSingleObjectiveFunction(
  name = &quot;xgb_cv_bayes&quot;,
  fn =   function(x){
    set.seed(12345)
    cv &lt;- xgb.cv(params = list(
      booster          = &quot;gbtree&quot;,
      eta              = x[&quot;eta&quot;],
      max_depth        = x[&quot;max_depth&quot;],
      min_child_weight = x[&quot;min_child_weight&quot;],
      gamma            = x[&quot;gamma&quot;],
      subsample        = x[&quot;subsample&quot;],
      colsample_bytree = x[&quot;colsample_bytree&quot;],
      objective        = &#39;count:poisson&#39;, 
      eval_metric     = &quot;poisson-nloglik&quot;),
      data = mydb_xgbmatrix,
      nround = 30,
      folds=  cv_folds,
      monotone_constraints = myConstraint$sens,
      prediction = FALSE,
      showsd = TRUE,
      early_stopping_rounds = 10,
      verbose = 0)
    
    cv$evaluation_log[, max(test_poisson_nloglik_mean)]
  },
  par.set = makeParamSet(
    makeNumericParam(&quot;eta&quot;,              lower = 0.001, upper = 0.05),
    makeNumericParam(&quot;gamma&quot;,            lower = 0,     upper = 5),
    makeIntegerParam(&quot;max_depth&quot;,        lower= 1,      upper = 10),
    makeIntegerParam(&quot;min_child_weight&quot;, lower= 1,      upper = 10),
    makeNumericParam(&quot;subsample&quot;,        lower = 0.2,   upper = 1),
    makeNumericParam(&quot;colsample_bytree&quot;, lower = 0.2,   upper = 1)
  ),
  minimize = FALSE
)</code></pre>
<p>After this, we generate the design, which are a set of hyperparameters that will be tested before starting the bayesian optimization. Here we generate only 10 sets, but <code>mlrMBO</code>would normally generate 4 times the number of parameters. I also force my <code>simon_params</code> to be part of the design because I want to make sure at least one of of sets generated is good.</p>
<pre class="r"><code># generate an optimal design with only 10  points
des = generateDesign(n=10,
                     par.set = getParamSet(obj.fun), 
                     fun = lhs::randomLHS)  ## . If no design is given by the user, mlrMBO will generate a maximin Latin Hypercube Design of size 4 times the number of the black-box function’s parameters.
# i still want my favorite hyperparameters to be tested
simon_params &lt;- data.frame(max_depth = 6,
                           colsample_bytree= 0.8,
                           subsample = 0.8,
                           min_child_weight = 3,
                           eta  = 0.01,
                           gamma = 0) %&gt;% as_tibble()
#final design  is a combination of latin hypercube optimization and my own preferred set of parameters
final_design =  simon_params  %&gt;% bind_rows(des)
# bayes will have 10 additional iterations
control = makeMBOControl()
control = setMBOControlTermination(control, iters = 10)</code></pre>
<p>Run the bayesian search:</p>
<pre class="r"><code>if ( switch_generate_interim_data){
  run = mbo(fun = obj.fun, 
            design = final_design,  
            control = control, 
            show.info = TRUE)
  write_rds( run, here::here(&quot;content/post/data/interim/run.rds&quot;))
} else {
  run &lt;- read_rds(here::here(&quot;content/post/data/interim/run.rds&quot;)) 
}</code></pre>
<pre class="r"><code># print a summary with run
#run
# return  best model hyperparameters using run$x
# return best log likelihood using run$y
# return all results using run$opt.path$env$path
run$opt.path$env$path  %&gt;% 
  mutate(Round = row_number()) %&gt;%
  mutate(type = case_when(
    Round==1  ~ &quot;1- hardcoded&quot;,
    Round&lt;= 11 ~ &quot;2 -design &quot;,
    TRUE ~ &quot;3 - mlrMBO optimization&quot;)) %&gt;%
  ggplot(aes(x= Round, y= y, color= type)) + 
  geom_point() +
  labs(title = &quot;mlrMBO optimization&quot;)+
  ylab(&quot;loglikelihood&quot;)</code></pre>
<p><img src="/post/2019-01-09-mlrmbo_poisson_files/figure-html/unnamed-chunk-8-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion:</h1>
<p>Bayesian optimization does generate better models, but it might be overkill if you aren’t participating in a kaggle. The table below shows the log likelihood of the best model found using the random grid and the mlrMBO and compares it my <code>simon params</code>.</p>
<table>
<thead>
<tr class="header">
<th align="left">type</th>
<th align="right">loglikelihood</th>
<th align="right">max_depth</th>
<th align="right">colsample_bytree</th>
<th align="right">subsample</th>
<th align="right">min_child_weight</th>
<th align="right">eta</th>
<th align="right">gamma</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">simon params</td>
<td align="right">0.5161797</td>
<td align="right">6</td>
<td align="right">0.8000000</td>
<td align="right">0.8000000</td>
<td align="right">3</td>
<td align="right">0.0100000</td>
<td align="right">0.000000</td>
</tr>
<tr class="even">
<td align="left">random_grid</td>
<td align="right">0.5169187</td>
<td align="right">3</td>
<td align="right">0.6796527</td>
<td align="right">0.8889676</td>
<td align="right">9</td>
<td align="right">0.0055361</td>
<td align="right">3.531973</td>
</tr>
<tr class="odd">
<td align="left">mlrMBO</td>
<td align="right">0.5176710</td>
<td align="right">2</td>
<td align="right">0.2768691</td>
<td align="right">0.6206134</td>
<td align="right">2</td>
<td align="right">0.0010027</td>
<td align="right">1.875530</td>
</tr>
</tbody>
</table>
</div>
<div id="code" class="section level1">
<h1>Code</h1>
<p>The code that generated this document is located at</p>
<p><a href="https://github.com/SimonCoulombe/snippets/blob/master/content/post/2019-1-09-bayesian.Rmd" class="uri">https://github.com/SimonCoulombe/snippets/blob/master/content/post/2019-1-09-bayesian.Rmd</a></p>
<div id="mlrmbo-further-reading" class="section level2">
<h2>mlrMBO further reading</h2>
<p>Here are some resources I used to build this post:</p>
<ul>
<li><a href="http://rstudio-pubs-static.s3.amazonaws.com/336732_52d1b0e682634b5eae42cf86e1fc2a98.html">Xgboost using MLR package</a><br />
</li>
<li><a href="https://mlrmbo.mlr-org.com/articles/supplementary/machine_learning_with_mlrmbo.html">Vignette: https://mlrmbo.mlr-org.com/articles/supplementary/machine_learning_with_mlrmbo.html</a> , how to use lrMBO with mlr. I dont think you can do poisson regression using mlr.</li>
<li><a href="https://www.r-bloggers.com/parameter-tuning-with-mlrhyperopt/">Parameter tuning with mlrHyperopt</a><br />
</li>
<li><a href="https://www.kaggle.com/xanderhorn/train-r-ml-models-efficiently-with-mlr">Train R ML models efficiently with mlr</a></li>
</ul>
</div>
</div>
<div id="appendix-not-recommended-bayesian-optimization-using-rbayesianoptimization" class="section level1">
<h1>Appendix: (NOT RECOMMENDED) Bayesian optimization using <code>rBayesianOptimization</code></h1>
<p>Here I show how to do an equivalent optimization using <code>rBaysianOptimization</code>. I do not recommend using this package because it <a href="https://github.com/yanyachen/rBayesianOptimization/issues/4">recycles hyperparameters</a> and hasnt been updated on github since 2016.</p>
<p>First, we have to define a special function that return a list of two values that will be returned to the bayesian optimiser:<br />
- “Score” should be the metrics to be maximized ,<br />
- “Pred” should be the validation/cross-valiation prediction for ensembling / stacking. We can set it to 0 to save on memory.</p>
<p>This function will be named <code>xgb_cv_bayes</code> :</p>
<pre class="r"><code>xgb_cv_bayes &lt;- 
  function(max_depth=4, 
           min_child_weight=1, 
           gamma=0,
           eta=0.01,
           subsample = 0.6,
           colsample_bytree =0.6,
           nrounds = 200, 
           early_stopping_rounds = 50 ){
    set.seed(1234)
    cv &lt;- xgb.cv(params = list(
      booster = &quot;gbtree&quot;,
      eta = eta,
      max_depth = max_depth,
      min_child_weight = min_child_weight,
      gamma = gamma,
      subsample = subsample,
      colsample_bytree = colsample_bytree,
      objective = &#39;count:poisson&#39;, 
      eval_metric = &quot;poisson-nloglik&quot;),
      data = mydb_xgbmatrix,
      nround = nrounds,
      folds=  cv_folds,
      monotone_constraints = myConstraint$sens,
      prediction = FALSE,
      showsd = TRUE,
      early_stopping_rounds = 50,
      verbose = 0)
    
    list(Score = cv$evaluation_log[, max(test_poisson_nloglik_mean)],
         Pred = 0)
  }</code></pre>
<p>We then launch <code>BayesianOptimization</code>, specifying:<br />
- the function to optimise (xgb_cv_bayes),<br />
- the hyperparameters (<code>bounds</code>) ,<br />
- the number of iterations, <code>n_iter</code>,</p>
<p>The optimization function needs some points for initialisation. We pass one of the two following parameters:<br />
- <code>init_points</code> , start from scratch using this number of randomly generated hyperparameter sets, or
- <code>init_grid_dt</code> use the knowledge from a previous run.</p>
<div id="rbayesianoptimization-from-scratch" class="section level2">
<h2>rBayesianOptimization from scratch</h2>
<p>Here is an example “from scratch”.</p>
<pre class="r"><code>if ( switch_generate_interim_data){
  bayesian_results &lt;- rBayesianOptimization::BayesianOptimization(
    FUN = xgb_cv_bayes,
    bounds = list(max_depth = c(2L, 10L),
                  colsample_bytree = c(0.3, 1),
                  subsample = c(0.3,1),
                  min_child_weight = c(1L, 10L),
                  eta = c(0.001, 0.03),
                  gamma = c(0, 5)),
    init_grid_dt = NULL, init_points = 4, 
    n_iter = 7,
    acq = &quot;ucb&quot;, kappa = 2.576, eps = 0.0,
    verbose = TRUE)
  
  write_rds(bayesian_results, here::here(&quot;content/post/data/interim/bayesian_results.rds&quot;))
} else {bayesian_results &lt;- read_rds( here::here(&quot;content/post/data/interim/bayesian_results.rds&quot;))}</code></pre>
<pre class="r"><code>knitr::kable(bayesian_results$History)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">Round</th>
<th align="right">max_depth</th>
<th align="right">colsample_bytree</th>
<th align="right">subsample</th>
<th align="right">min_child_weight</th>
<th align="right">eta</th>
<th align="right">gamma</th>
<th align="right">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">10</td>
<td align="right">0.3968879</td>
<td align="right">0.5821038</td>
<td align="right">1</td>
<td align="right">0.0103839</td>
<td align="right">2.0822446</td>
<td align="right">0.5161180</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">3</td>
<td align="right">0.8768590</td>
<td align="right">0.7366128</td>
<td align="right">9</td>
<td align="right">0.0135183</td>
<td align="right">4.8732243</td>
<td align="right">0.5155987</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">3</td>
<td align="right">0.7481389</td>
<td align="right">0.5984350</td>
<td align="right">8</td>
<td align="right">0.0034476</td>
<td align="right">0.2085726</td>
<td align="right">0.5172650</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">7</td>
<td align="right">0.6551866</td>
<td align="right">0.9697193</td>
<td align="right">5</td>
<td align="right">0.0045494</td>
<td align="right">1.1913145</td>
<td align="right">0.5170820</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">4</td>
<td align="right">0.9726352</td>
<td align="right">0.7399260</td>
<td align="right">4</td>
<td align="right">0.0010000</td>
<td align="right">1.4119299</td>
<td align="right">0.5176710</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">4</td>
<td align="right">0.9726295</td>
<td align="right">0.7399255</td>
<td align="right">4</td>
<td align="right">0.0010000</td>
<td align="right">1.4119589</td>
<td align="right">0.5176710</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">4</td>
<td align="right">0.9726215</td>
<td align="right">0.7399260</td>
<td align="right">4</td>
<td align="right">0.0010000</td>
<td align="right">1.4119589</td>
<td align="right">0.5176710</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">4</td>
<td align="right">0.9726190</td>
<td align="right">0.7399260</td>
<td align="right">4</td>
<td align="right">0.0010000</td>
<td align="right">1.4119589</td>
<td align="right">0.5176710</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">4</td>
<td align="right">0.9726177</td>
<td align="right">0.7399260</td>
<td align="right">4</td>
<td align="right">0.0010000</td>
<td align="right">1.4119589</td>
<td align="right">0.5176710</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">4</td>
<td align="right">0.9726167</td>
<td align="right">0.7399260</td>
<td align="right">4</td>
<td align="right">0.0010000</td>
<td align="right">1.4119589</td>
<td align="right">0.5176710</td>
</tr>
<tr class="odd">
<td align="right">11</td>
<td align="right">5</td>
<td align="right">0.3701890</td>
<td align="right">0.8769125</td>
<td align="right">8</td>
<td align="right">0.0299810</td>
<td align="right">3.9359835</td>
<td align="right">0.5128937</td>
</tr>
</tbody>
</table>
<pre class="r"><code>bayesian_results$History  %&gt;% 
  mutate(type = case_when(
    Round&lt;= 4  ~ &quot;1- init_points&quot;,
    Round&lt;= 11 ~ &quot;2 -n_iter&quot;,
    TRUE ~ &quot;wtf&quot;)) %&gt;%
  ggplot(aes(x= Round, y= Value, color= type)) + geom_point()+
  labs(title = &quot;bayesian from scratch&quot;)</code></pre>
<p><img src="/post/2019-01-09-mlrmbo_poisson_files/figure-html/unnamed-chunk-11-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="rbayesianoptimization-resuming-from-previous-runs" class="section level2">
<h2>rBayesianOptimization resuming from previous runs</h2>
<p><code>init_grid_dt</code> is a data frame with the same columns as <code>bounds</code> plus a <code>Value</code>column which correspond to the log likelihood we calculated earlier.</p>
<div id="resuming-from-random-grid" class="section level3">
<h3>… resuming from random grid</h3>
<p>Here is an example resuming from the random grid of example #1</p>
<pre class="r"><code>init_grid &lt;- random_grid_results$data %&gt;% 
  bind_rows() %&gt;% 
  add_column(Value = random_grid_results$Score)   %&gt;% 
  select(max_depth, colsample_bytree, subsample, min_child_weight, eta, gamma, Value)
knitr::kable(init_grid)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">max_depth</th>
<th align="right">colsample_bytree</th>
<th align="right">subsample</th>
<th align="right">min_child_weight</th>
<th align="right">eta</th>
<th align="right">gamma</th>
<th align="right">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.8000000</td>
<td align="right">0.8000000</td>
<td align="right">3</td>
<td align="right">0.0100000</td>
<td align="right">0.000000</td>
<td align="right">0.5161797</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">0.3697140</td>
<td align="right">0.7176482</td>
<td align="right">3</td>
<td align="right">0.0447509</td>
<td align="right">0.000000</td>
<td align="right">0.5104867</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0.7213390</td>
<td align="right">0.8263462</td>
<td align="right">5</td>
<td align="right">0.0259997</td>
<td align="right">0.000000</td>
<td align="right">0.5135480</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.3004441</td>
<td align="right">0.6424290</td>
<td align="right">4</td>
<td align="right">0.0215211</td>
<td align="right">0.000000</td>
<td align="right">0.5142803</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.4137765</td>
<td align="right">0.6237757</td>
<td align="right">7</td>
<td align="right">0.0474252</td>
<td align="right">0.000000</td>
<td align="right">0.5100597</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.5088913</td>
<td align="right">0.8314850</td>
<td align="right">3</td>
<td align="right">0.0141615</td>
<td align="right">0.000000</td>
<td align="right">0.5154927</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.2107123</td>
<td align="right">0.2186650</td>
<td align="right">5</td>
<td align="right">0.0446673</td>
<td align="right">0.000000</td>
<td align="right">0.5104790</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.5059104</td>
<td align="right">0.5817841</td>
<td align="right">8</td>
<td align="right">0.0093015</td>
<td align="right">0.000000</td>
<td align="right">0.5162973</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.8957527</td>
<td align="right">0.7858510</td>
<td align="right">1</td>
<td align="right">0.0167293</td>
<td align="right">0.000000</td>
<td align="right">0.5150687</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.4722792</td>
<td align="right">0.7541852</td>
<td align="right">9</td>
<td align="right">0.0105983</td>
<td align="right">0.000000</td>
<td align="right">0.5160807</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.5856641</td>
<td align="right">0.5820957</td>
<td align="right">4</td>
<td align="right">0.0163778</td>
<td align="right">0.000000</td>
<td align="right">0.5151290</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.6796527</td>
<td align="right">0.8889676</td>
<td align="right">9</td>
<td align="right">0.0055361</td>
<td align="right">3.531973</td>
<td align="right">0.5169187</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.5948330</td>
<td align="right">0.5504777</td>
<td align="right">4</td>
<td align="right">0.0405373</td>
<td align="right">2.702602</td>
<td align="right">0.5111790</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">0.3489741</td>
<td align="right">0.3958378</td>
<td align="right">4</td>
<td align="right">0.0545762</td>
<td align="right">9.926841</td>
<td align="right">0.5088790</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.8618987</td>
<td align="right">0.2565432</td>
<td align="right">5</td>
<td align="right">0.0487349</td>
<td align="right">6.334933</td>
<td align="right">0.5098037</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">0.7347734</td>
<td align="right">0.2795729</td>
<td align="right">9</td>
<td align="right">0.0498385</td>
<td align="right">2.132081</td>
<td align="right">0.5096243</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.8353919</td>
<td align="right">0.4530174</td>
<td align="right">9</td>
<td align="right">0.0293165</td>
<td align="right">1.293724</td>
<td align="right">0.5130033</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.2863549</td>
<td align="right">0.6149074</td>
<td align="right">4</td>
<td align="right">0.0266050</td>
<td align="right">4.781180</td>
<td align="right">0.5134483</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.7789688</td>
<td align="right">0.7296041</td>
<td align="right">8</td>
<td align="right">0.0506522</td>
<td align="right">9.240745</td>
<td align="right">0.5095290</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.5290195</td>
<td align="right">0.5254641</td>
<td align="right">10</td>
<td align="right">0.0382960</td>
<td align="right">5.987610</td>
<td align="right">0.5115423</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.8567570</td>
<td align="right">0.9303007</td>
<td align="right">5</td>
<td align="right">0.0412834</td>
<td align="right">9.761707</td>
<td align="right">0.5110460</td>
</tr>
</tbody>
</table>
<pre class="r"><code>if (switch_generate_interim_data){
  bayesian_results_continued_from_randomgrid &lt;- 
    BayesianOptimization(xgb_cv_bayes,
                         bounds = list(max_depth = c(2L, 10L),
                                       colsample_bytree = c(0.3, 1),
                                       subsample = c(0.3,1),
                                       min_child_weight = c(1L, 10L),
                                       eta = c(0.001, 0.03),
                                       gamma = c(0, 5)),
                         init_grid_dt = init_grid, init_points = 0, 
                         n_iter = 5,
                         acq = &quot;ucb&quot;, kappa = 2.576, eps = 0.0,
                         verbose = TRUE)
  
  write_rds(bayesian_results_continued_from_randomgrid,
            here::here(&quot;content/post/data/interim/bayesian_results_continued_from_randomgrid.rds&quot;))
} else {
  
  bayesian_results_continued_from_randomgrid &lt;- read_rds(
    here::here(&quot;content/post/data/interim/bayesian_results_continued_from_randomgrid.rds&quot;))
}</code></pre>
</div>
<div id="resuming-from-previous-rbayesianoptimization" class="section level3">
<h3>… resuming from previous rBayesianOptimization</h3>
<p>We can also resume from a brevious rBayesianOptimization run using the <code>$History</code> value it returns. In the example below, we will resume from the previous bayesian search that was itself build upon the random grid search.</p>
<pre class="r"><code>init_grid &lt;- bayesian_results_continued_from_randomgrid$History %&gt;% select(-Round)
knitr::kable(init_grid)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">max_depth</th>
<th align="right">colsample_bytree</th>
<th align="right">subsample</th>
<th align="right">min_child_weight</th>
<th align="right">eta</th>
<th align="right">gamma</th>
<th align="right">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.8000000</td>
<td align="right">0.8000000</td>
<td align="right">3</td>
<td align="right">0.0100000</td>
<td align="right">0.0000000</td>
<td align="right">0.5161797</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">0.3697140</td>
<td align="right">0.7176482</td>
<td align="right">3</td>
<td align="right">0.0447509</td>
<td align="right">0.0000000</td>
<td align="right">0.5104867</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0.7213390</td>
<td align="right">0.8263462</td>
<td align="right">5</td>
<td align="right">0.0259997</td>
<td align="right">0.0000000</td>
<td align="right">0.5135480</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.3004441</td>
<td align="right">0.6424290</td>
<td align="right">4</td>
<td align="right">0.0215211</td>
<td align="right">0.0000000</td>
<td align="right">0.5142803</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.4137765</td>
<td align="right">0.6237757</td>
<td align="right">7</td>
<td align="right">0.0474252</td>
<td align="right">0.0000000</td>
<td align="right">0.5100597</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.5088913</td>
<td align="right">0.8314850</td>
<td align="right">3</td>
<td align="right">0.0141615</td>
<td align="right">0.0000000</td>
<td align="right">0.5154927</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.2107123</td>
<td align="right">0.2186650</td>
<td align="right">5</td>
<td align="right">0.0446673</td>
<td align="right">0.0000000</td>
<td align="right">0.5104790</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.5059104</td>
<td align="right">0.5817841</td>
<td align="right">8</td>
<td align="right">0.0093015</td>
<td align="right">0.0000000</td>
<td align="right">0.5162973</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.8957527</td>
<td align="right">0.7858510</td>
<td align="right">1</td>
<td align="right">0.0167293</td>
<td align="right">0.0000000</td>
<td align="right">0.5150687</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.4722792</td>
<td align="right">0.7541852</td>
<td align="right">9</td>
<td align="right">0.0105983</td>
<td align="right">0.0000000</td>
<td align="right">0.5160807</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.5856641</td>
<td align="right">0.5820957</td>
<td align="right">4</td>
<td align="right">0.0163778</td>
<td align="right">0.0000000</td>
<td align="right">0.5151290</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.6796527</td>
<td align="right">0.8889676</td>
<td align="right">9</td>
<td align="right">0.0055361</td>
<td align="right">3.5319727</td>
<td align="right">0.5169187</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.5948330</td>
<td align="right">0.5504777</td>
<td align="right">4</td>
<td align="right">0.0405373</td>
<td align="right">2.7026015</td>
<td align="right">0.5111790</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">0.3489741</td>
<td align="right">0.3958378</td>
<td align="right">4</td>
<td align="right">0.0545762</td>
<td align="right">9.9268406</td>
<td align="right">0.5088790</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.8618987</td>
<td align="right">0.2565432</td>
<td align="right">5</td>
<td align="right">0.0487349</td>
<td align="right">6.3349326</td>
<td align="right">0.5098040</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">0.7347734</td>
<td align="right">0.2795729</td>
<td align="right">9</td>
<td align="right">0.0498385</td>
<td align="right">2.1320814</td>
<td align="right">0.5096243</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.8353919</td>
<td align="right">0.4530174</td>
<td align="right">9</td>
<td align="right">0.0293165</td>
<td align="right">1.2937235</td>
<td align="right">0.5130033</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.2863549</td>
<td align="right">0.6149074</td>
<td align="right">4</td>
<td align="right">0.0266050</td>
<td align="right">4.7811803</td>
<td align="right">0.5134483</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.7789688</td>
<td align="right">0.7296041</td>
<td align="right">8</td>
<td align="right">0.0506522</td>
<td align="right">9.2407447</td>
<td align="right">0.5095290</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.5290195</td>
<td align="right">0.5254641</td>
<td align="right">10</td>
<td align="right">0.0382960</td>
<td align="right">5.9876097</td>
<td align="right">0.5115423</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.8567570</td>
<td align="right">0.9303007</td>
<td align="right">5</td>
<td align="right">0.0412834</td>
<td align="right">9.7617069</td>
<td align="right">0.5110460</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.8558321</td>
<td align="right">0.8749248</td>
<td align="right">5</td>
<td align="right">0.0071978</td>
<td align="right">4.4704073</td>
<td align="right">0.5166430</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">0.4646962</td>
<td align="right">0.5923405</td>
<td align="right">7</td>
<td align="right">0.0053862</td>
<td align="right">0.2069889</td>
<td align="right">0.5169443</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.3418069</td>
<td align="right">0.8590210</td>
<td align="right">2</td>
<td align="right">0.0035004</td>
<td align="right">2.8826489</td>
<td align="right">0.5172560</td>
</tr>
<tr class="odd">
<td align="right">10</td>
<td align="right">0.4647044</td>
<td align="right">0.6429887</td>
<td align="right">7</td>
<td align="right">0.0049578</td>
<td align="right">5.0000000</td>
<td align="right">0.5170143</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0.7317249</td>
<td align="right">0.5751665</td>
<td align="right">8</td>
<td align="right">0.0010000</td>
<td align="right">4.7603658</td>
<td align="right">0.5176710</td>
</tr>
</tbody>
</table>
<pre class="r"><code>if (switch_generate_interim_data){
  bayesian_results_continued_from_bayesian &lt;- 
    BayesianOptimization(xgb_cv_bayes,
                         bounds = list(max_depth = c(2L, 10L),
                                       colsample_bytree = c(0.3, 1),
                                       subsample = c(0.3,1),
                                       min_child_weight = c(1L, 10L),
                                       eta = c(0.001, 0.03),
                                       gamma = c(0, 5)),
                         init_grid_dt =init_grid , init_points = 0,
                         n_iter = 5,
                         acq = &quot;ucb&quot;, kappa = 2.576, eps = 0.0,
                         verbose = TRUE)
  write_rds(bayesian_results_continued_from_bayesian,here::here(&quot;content/post/data/interim/bayesian_results_continued_from_bayesian.rds&quot;))
} else {
  bayesian_results_continued_from_bayesian &lt;- read_rds(
    here::here(&quot;content/post/data/interim/bayesian_results_continued_from_bayesian.rds&quot;))
}  </code></pre>
<pre class="r"><code>bayesian_results_continued_from_bayesian$History  %&gt;% 
  mutate(type = case_when(
    Round==1  ~ &quot;1- hardcoded&quot;,
    Round&lt;= 21 ~ &quot;2 -random grid &quot;,
    Round &lt;= 26 ~ &quot;3 - rbayesianoptimization run #1 resuming from random grid&quot;,
    TRUE ~ &quot;4 - rbayesianoptimization run #2 resuming from bayesian run 1&quot;)) %&gt;%
  ggplot(aes(x= Round, y= Value, color= type)) + 
  geom_point()+ 
  labs(title = &quot;rBayesianOptimization&quot;)+
  ylab(&quot;loglikelihood&quot;)</code></pre>
<p><img src="/post/2019-01-09-mlrmbo_poisson_files/figure-html/results-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="conclusion-1" class="section level1">
<h1>Conclusion</h1>
<p>The table below shows the loglikelihood and hyperparameters for the hardcoded <code>simon_param</code> model, and the best models found by random search, mlrMBO and rBayesianOptimisation.</p>
<p>Bayesian optimization using mlrMBO and rBayesianOptimization both yield the best results. The gain in loglikelihood above the “hardcoded” <code>simon_param</code> or the random search isnt that great, however, so it may not be necessary to implement <code>mlrMBO</code> in a non-kaggle setting.</p>
<pre class="r"><code>hardcoded &lt;- random_grid_results[1,] %&gt;%
  pull(data) %&gt;% .[[1]] %&gt;%
  mutate(type = &quot;hardcoded&quot;) %&gt;%
  mutate(loglikelihood= random_grid_results[1,&quot;Score&quot;] %&gt;% as.numeric())
best_random &lt;- random_grid_results %&gt;% 
  filter(Score == max(Score)) %&gt;% pull(data) %&gt;% .[[1]] %&gt;% 
  mutate(type = &quot;random_grid&quot;) %&gt;%
  mutate(loglikelihood = max(random_grid_results$Score))

best_mlrMBO &lt;- run$opt.path$env$path  %&gt;% 
  filter(y == max(y))  %&gt;%
  mutate(type = &quot;mlrMBO&quot;) %&gt;%
  mutate(loglikelihood= run$y ) %&gt;%
  head(1)
best_rBayesianOptimization &lt;- 
  bayesian_results_continued_from_bayesian$History %&gt;%
  filter(Value == max(Value)) %&gt;%
  mutate(type = &quot;rBayesianOptimization&quot;) %&gt;%
  mutate(loglikelihood = max(bayesian_results_continued_from_bayesian$History$Value))
hardcoded %&gt;% 
  bind_rows(best_random) %&gt;% 
  bind_rows(best_mlrMBO) %&gt;%
  bind_rows(best_rBayesianOptimization) %&gt;%
  select(type,loglikelihood, max_depth, colsample_bytree, subsample,min_child_weight,eta, gamma) %&gt;% knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">type</th>
<th align="right">loglikelihood</th>
<th align="right">max_depth</th>
<th align="right">colsample_bytree</th>
<th align="right">subsample</th>
<th align="right">min_child_weight</th>
<th align="right">eta</th>
<th align="right">gamma</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">hardcoded</td>
<td align="right">0.5161797</td>
<td align="right">6</td>
<td align="right">0.8000000</td>
<td align="right">0.8000000</td>
<td align="right">3</td>
<td align="right">0.0100000</td>
<td align="right">0.000000</td>
</tr>
<tr class="even">
<td align="left">random_grid</td>
<td align="right">0.5169187</td>
<td align="right">3</td>
<td align="right">0.6796527</td>
<td align="right">0.8889676</td>
<td align="right">9</td>
<td align="right">0.0055361</td>
<td align="right">3.531973</td>
</tr>
<tr class="odd">
<td align="left">mlrMBO</td>
<td align="right">0.5176710</td>
<td align="right">2</td>
<td align="right">0.2768691</td>
<td align="right">0.6206134</td>
<td align="right">2</td>
<td align="right">0.0010027</td>
<td align="right">1.875530</td>
</tr>
<tr class="even">
<td align="left">rBayesianOptimization</td>
<td align="right">0.5176710</td>
<td align="right">2</td>
<td align="right">0.7317249</td>
<td align="right">0.5751665</td>
<td align="right">8</td>
<td align="right">0.0010000</td>
<td align="right">4.760366</td>
</tr>
<tr class="odd">
<td align="left">rBayesianOptimization</td>
<td align="right">0.5176710</td>
<td align="right">10</td>
<td align="right">1.0000000</td>
<td align="right">0.3000000</td>
<td align="right">1</td>
<td align="right">0.0010000</td>
<td align="right">3.249263</td>
</tr>
</tbody>
</table>
</div>
