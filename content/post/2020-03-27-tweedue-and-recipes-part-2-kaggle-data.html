---
title: Tweedue and Recipes part 2 (kaggle data)
author: simon
date: '2020-03-27'
slug: tweedue-and-recipes-part-2-kaggle-data
categories: []
tags: []
keywords:
  - tech
---



<p>I just got my feet wet with tweedie regression and the recipes package <a href="https://www.simoncoulombe.com/2020/03/tweedie-vs-poisson-gamma/">yesterday</a>. The results have been underwhelming, as the models didnt appear that predictive. I figured I might give it another try, this time using <a href="https://www.kaggle.com/c/ClaimPredictionChallenge">Kaggle’s claim prediction challenge</a> from 2012.</p>
<p>It is no longer possible to submit models, so we will create our own 20% test sample from the kaggle training data set and see how we fare. Submissions were evaluated on the Normalized Gini coefficient.</p>
<p><img src="/post/2020-03-27-tweedue-and-recipes-part-2-kaggle-data_files/leaderboard_kaggle.png" /></p>
<div id="overview-of-the-approach" class="section level1">
<h1>Overview of the approach:</h1>
<p>We split the train data (13M obs, 35 columns) into 80% train and 20% test. We will use k-fold cross validation to find the best number of iterations for an xgboost tweedie model using all available columns but no feature engineering.
We will train a new model on the full 80% training dataset. The 20% test dataset will then be scored and the model prediction will be evaluated using normalized gini, as was done in the competition.</p>
<p>I don’t do anything clever such as feature engineering or capping individual losses, so I dont expect a very good leaderboard performance. That being said, this is a very old competition from 2012, so they didnt have access to xgboost… let’s see!</p>
<p>My 32GB of RAM was a bit short to work with the data. I saved a lot to disk, removing objects that weren’t absolutely necessary.</p>
<p>Corners were slightly cut, such as:</p>
<ul>
<li>The prep() function that trains all the knn imputation models was only run on a 1e5 of the 10M records.<br />
</li>
<li>The k-fold cross validation only had 3 folds and was performed on 3e6 observations. The best iteration was 310.<br />
</li>
<li>The full model was trained on 6e6 observations (instead of 10e6).</li>
</ul>
</div>
<div id="libraries" class="section level1">
<h1>Libraries</h1>
<p>The usual tidyverse suspects are used for data wrangling and plots. I use themes and palettes from Claus Wilke’s <code>dviz.supp</code> and <code>colorblindr</code> packages. Plots in the lift charts are arranged in a grid using the <code>patchwork</code> package.<br />
The pre-processing pipeline uses <code>recipes</code>, <code>rsample</code> and <code>yardstick</code>.<br />
<code>Vroom</code> is used to read the csv. <code>fst</code>is used to save and read intermediate outputs.<br />
<code>MLmetrics</code>is used to compute the normalised gini coefficient.</p>
<pre class="r"><code>library(tidyverse)  # used for data wrangling
library(xgboost) # used for modelling
library(tictoc) # to time execution
library(dviz.supp) # devtools::install_github(&quot;clauswilke/dviz.supp&quot;)
library(colorblindr)  # devtools::install_github(&quot;clauswilke/colorblindr&quot;)
library(patchwork) # to arrange plots in a grid
library(rsample) # create train/test sample
library(yardstick)
library(recipes)
library(vroom) # for quick reading of csv
library(fst) # for quick saving of tables
library(MLmetrics) # for normalized gini </code></pre>
<pre class="r"><code># kaggle claim data from https://www.kaggle.com/c/ClaimPredictionChallenge/daatre
kaggle_train &lt;- vroom::vroom(here::here(&quot;content/post/data/downloads/train_set.csv&quot;)) %&gt;%
  select(-Row_ID, -Household_ID, -Vehicle)


set.seed(42)
#https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html
split &lt;- rsample::initial_split(kaggle_train, prop = 0.8)
train &lt;- rsample::training(split)
test &lt;- rsample::testing(split)

train_1e5_split &lt;-  rsample::initial_split(train, prop = 0.01) # 1e5 obs used for prepping recipe
train_1e5 &lt;- rsample::training(train_1e5_split)


fst::write_fst(train, &quot;train.fst&quot;)
fst::write_fst(train_1e5, &quot;train_1e5.fst&quot;)
fst::write_fst(test, &quot;test.fst&quot;)


rm(list=ls()) # clear memory of object but keep libraries.
invisible(gc())</code></pre>
</div>
<div id="prepare-recipe" class="section level1">
<h1>Prepare recipe</h1>
<p>Here we only load the train dataset because of memory issues.</p>
<p>step_string2factor(all_nominal()) is required because step_knnimpute() doesnt work with character columns.</p>
<p>I used head(1e4) to “train” the recipe because otherwise it take even longer. I was concerned of doing this: what happens if a factor isnt present in the first 1e4 rows? Will it crash? I did a few tests and unknown factor levels are considered as NA and imputed using knn. It’s <strong>nice</strong>!.</p>
<pre class="r"><code>tic()
train &lt;- read_fst(&quot;train_1e5.fst&quot;) # , from =1, to = 1e6


rec &lt;-  recipes::recipe(Claim_Amount ~ ., 
                        train %&gt;% head(1e5)) %&gt;% 
  recipes::step_zv(recipes::all_predictors()) %&gt;%   # remove variable with all equal values
  step_string2factor(all_nominal()) %&gt;% # doesnt lke character columns
  step_mutate(
    Model_Year = as.factor(Model_Year),
    Calendar_Year   = as.factor(Calendar_Year)
  ) %&gt;%
  recipes::step_other(recipes::all_nominal() , threshold = 0.01)  %&gt;%       # combine categories with less than 1% of observation
  step_knnimpute(all_predictors()) %&gt;%
  recipes::step_dummy(recipes::all_nominal())  %&gt;% # convert to dummy for xgboost use
  check_missing(all_predictors()) ## break the bake function if any of the checked columns contains NA value

# Prepare the recipe and use juice/bake to get the d2ata!
trained_rec &lt;- prep(rec)

write_rds(trained_rec, &quot;trained_rec.rds&quot;)

rm(train_1e5)


train &lt;- read_fst(&quot;train.fst&quot;)
train &lt;- bake(trained_rec, new_data =   train )




write_fst(train, &quot;baked_train.fst&quot;)


toc() # 190.161 sec elapsed for 1e6 train , 1e4prep   # 2017.908 sec elapsed sur train complet (10M) et prep 1e5

rm(list=ls()) # clear memory of object but keep libraries.
invisible(gc())</code></pre>
<pre class="r"><code>train &lt;- read_fst(&quot;baked_train.fst&quot;)

train_3e6_split &lt;-  rsample::initial_split(train, prop = 0.3)

train_3e6 &lt;- rsample::training(train_3e6_split)
fst::write_fst(train_3e6, &quot;baked_train_3e6.fst&quot;)
rm(train3e6)
rm(train3e6_split)
train_6e6_split &lt;-  rsample::initial_split(train, prop = 0.6)
train_6e6 &lt;- rsample::training(train_6e6_split)

fst::write_fst(train_6e6, &quot;baked_train_6e6.fst&quot;)



rm(list=ls()) # clear memory of object but keep libraries.
invisible(gc())</code></pre>
</div>
<div id="train-xgboost" class="section level1">
<h1>Train XGBoost</h1>
<div id="find-best-number-of-iterations" class="section level2">
<h2>find best number of iterations</h2>
<pre class="r"><code>train &lt;- read_fst( &quot;baked_train_3e6.fst&quot;, from = 1, to = 6e6)

xgtrain &lt;- xgb.DMatrix(as.matrix(train %&gt;% select(-Claim_Amount)),  
                       label = train$Claim_Amount
)
rm(train) # save memory
tic()
params &lt;-list(
  booster = &quot;gbtree&quot;,
  objective = &#39;reg:tweedie&#39;,
  eval_metric = &quot;tweedie-nloglik@1.1&quot;,
  tweedie_variance_power = 1.1,
  gamma = 0,
  max_depth = 4,
  eta = 0.01,
  min_child_weight = 3,
  subsample = 0.8,
  colsample_bytree = 0.8,
  tree_method = &quot;hist&quot;
)

xgcv &lt;- xgb.cv(
  params = params,
  data = xgtrain,
  nround = 500,
  nfold=  3,
  showsd = TRUE,
  early_stopping_rounds = 50)


best_iter &lt;- xgcv$best_iteration # 310, 3 folds, 3e6 obs
write_rds(best_iter, &quot;best_iter.rds&quot;)

rm(list=ls()) # clear memory of object but keep libraries.
invisible(gc())


toc()</code></pre>
<pre class="r"><code>tic()
train &lt;- read_fst( &quot;baked_train_6e6.fst&quot;, from = 1, to = 6e6)

xgtrain &lt;- xgb.DMatrix(as.matrix(train %&gt;% select(-Claim_Amount)),  
                       label = train$Claim_Amount
)
rm(train) # save memory

best_iter &lt;- read_rds(&quot;best_iter.rds&quot;)


params &lt;-list(
  booster = &quot;gbtree&quot;,
  objective = &#39;reg:tweedie&#39;,
  eval_metric = &quot;tweedie-nloglik@1.1&quot;,
  tweedie_variance_power = 1.1,
  gamma = 0,
  max_depth = 4,
  eta = 0.01,
  min_child_weight = 3,
  subsample = 0.8,
  colsample_bytree = 0.8,
  tree_method = &quot;hist&quot;
)
# xgb.cv doesnt ouput any model -- we need a model to predict test dataset
xgmodel &lt;- xgboost::xgb.train(
  data = xgtrain,
  params = params,
  nrounds =best_iter#, # = #310 
  #nrounds = 50,
  #nthread = parallel::detectCores() - 1
)

write_rds(xgmodel, &quot;xgmodel.rds&quot;)


rm(list=ls()) # clear memory of object but keep libraries.
invisible(gc())

toc()</code></pre>
<pre><code>## 269.068 sec elapsed</code></pre>
</div>
</div>
<div id="apply-recipe-to-test-data" class="section level1">
<h1>Apply recipe to test data</h1>
<pre class="r"><code>trained_rec &lt;- read_rds(&quot;trained_rec.rds&quot;)
test &lt;- fst::read_fst( &quot;test.fst&quot;)
test &lt;-  bake(trained_rec, new_data = test)
write_fst(test, &quot;baked_test.fst&quot;)

rm(list=ls()) # clear memory of object but keep libraries.
invisible(gc())</code></pre>
</div>
<div id="predict-xgboost" class="section level1">
<h1>Predict XGBoost</h1>
<pre class="r"><code>xgmodel &lt;- read_rds(&quot;xgmodel.rds&quot;)
test &lt;- read_fst( &quot;baked_test.fst&quot;)
xgtest &lt;- xgb.DMatrix(as.matrix(test  %&gt;% select(-Claim_Amount)),  
                      label = test$Claim_Amount
)


test_w_preds &lt;-
  test %&gt;%
  mutate(pred_claim_xgboost = predict(xgmodel, xgtest)) %&gt;%
  mutate(exposure = 1)



rm(test)
mean(test_w_preds$Claim_Amount) # 1.334191</code></pre>
<pre><code>## [1] 1.334191</code></pre>
<pre class="r"><code>mean(test_w_preds$pred_claim_xgboost) #  1.331216</code></pre>
<pre><code>## [1] 1.283746</code></pre>
<pre class="r"><code>MLmetrics::NormalizedGini(test_w_preds$pred_claim_xgboost, test_w_preds$Claim_Amount) # 0.127837    would have ranked 16 / 102 back in 2012.. not excellent .. but it works</code></pre>
<pre><code>## [1] 0.1190057</code></pre>
<pre class="r"><code>#&#39; @title add_equal_weight_group()
#&#39;
#&#39; @description Cette fonction crée des groupe (quantiles) avec le nombre nombre total d&#39;exposition.
#&#39; @param table data.frame  source
#&#39; @param sort_by Variable utilisée pour trier les observations.
#&#39; @param expo Exposition (utilisée pour créer des quantiles de la même taille.  Si NULL, l&#39;exposition est égale pour toutes les observations) (Défault = NULL).
#&#39; @param nb Nombre de quantiles crées (défaut = 10)
#&#39; @param group_variable_name Nom de la variable de groupes créée
#&#39; @export


add_equal_weight_group &lt;- function(table, sort_by, expo = NULL, group_variable_name = &quot;groupe&quot;, nb = 10) {
  sort_by_var &lt;- enquo(sort_by)
  groupe_variable_name_var &lt;- enquo(group_variable_name)
  
  if (!(missing(expo))){ # https://stackoverflow.com/questions/48504942/testing-a-function-that-uses-enquo-for-a-null-parameter
    
    expo_var &lt;- enquo(expo)
    
    total &lt;- table %&gt;% pull(!!expo_var) %&gt;% sum
    br &lt;- seq(0, total, length.out = nb + 1) %&gt;% head(-1) %&gt;% c(Inf) %&gt;% unique
    table %&gt;%
      arrange(!!sort_by_var) %&gt;%
      mutate(cumExpo = cumsum(!!expo_var)) %&gt;%
      mutate(!!group_variable_name := cut(cumExpo, breaks = br, ordered_result = TRUE, include.lowest = TRUE) %&gt;% as.numeric) %&gt;%
      select(-cumExpo)
  } else {
    total &lt;- nrow(table)
    br &lt;- seq(0, total, length.out = nb + 1) %&gt;% head(-1) %&gt;% c(Inf) %&gt;% unique
    table %&gt;%
      arrange(!!sort_by_var) %&gt;%
      mutate(cumExpo = row_number()) %&gt;%
      mutate(!!group_variable_name := cut(cumExpo, breaks = br, ordered_result = TRUE, include.lowest = TRUE) %&gt;% as.numeric) %&gt;%
      select(-cumExpo)
  }
}

get_lift_chart_data &lt;- function(
  data, 
  sort_by,
  pred, 
  expo, 
  obs, 
  nb = 10) {
  
  pred_var &lt;- enquo(pred)
  sort_by_var &lt;- enquo(sort_by)
  expo_var &lt;- enquo(expo)
  obs_var &lt;- enquo(obs)
  
  
  pred_name &lt;- quo_name(pred_var)
  sort_by_name &lt;- quo_name(sort_by_var)
  obs_name &lt;- quo_name(obs_var)
  
  # constitution des buckets de poids égaux
  dd &lt;- data %&gt;% add_equal_weight_group(
    sort_by = !!sort_by_var,
    expo = !!expo_var, 
    group_variable_name = &quot;groupe&quot;,
    nb = nb
  )
  
  # comparaison sur ces buckets
  dd &lt;- full_join(
    dd %&gt;% 
      group_by(groupe) %&gt;%
      summarise(
        exposure = sum(!!expo_var),
        sort_by_moyen = mean(!!sort_by_var),
        sort_by_min = min(!!sort_by_var),
        sort_by_max = max(!!sort_by_var)
      ) %&gt;%
      ungroup(),
    dd %&gt;% 
      group_by(groupe) %&gt;%
      summarise_at(
        funs(sum(.) / sum(!!expo_var)),
        .vars = vars(!!obs_var, !!pred_var)
      ) %&gt;%
      ungroup,
    by = &quot;groupe&quot;
  )
  
  # création des labels
  dd &lt;- dd %&gt;%
    mutate(labs = paste0(&quot;[&quot;, round(sort_by_min, 2), &quot;, &quot;, round(sort_by_max, 2), &quot;]&quot;))
  
}



get_lift_chart &lt;- function(data, 
                           sort_by,
                           pred, 
                           expo, 
                           obs, 
                           nb){
  
  pred_var &lt;- enquo(pred)
  sort_by_var &lt;- enquo(sort_by)
  expo_var &lt;- enquo(expo)
  obs_var &lt;- enquo(obs)
  
  lift_data &lt;- get_lift_chart_data(
    data = data, 
    sort_by = !!sort_by_var,
    pred = !!pred_var, 
    expo = !!expo_var, 
    obs = !!obs_var, 
    nb = 10)
  
  p1 &lt;- lift_data %&gt;% 
    mutate(groupe = as.factor(groupe)) %&gt;%
    select(groupe, labs, !!pred_var, !!obs_var) %&gt;%
    gather(key = type, value = average, !!pred_var, !!obs_var)  %&gt;% 
    ggplot(aes(x= groupe, y = average, color =type, group = type)) +
    geom_line() +
    geom_point()+
    cowplot::theme_half_open() +
    cowplot::background_grid() +
    colorblindr::scale_color_OkabeIto( ) + 
    # scale_y_continuous(
    #   breaks  = scales::pretty_breaks()
    # )+
    theme(
      legend.position = c(0.1, 0.8),
      axis.title.x=element_blank(),
      axis.text.x=element_blank(),
      axis.ticks.x=element_blank()
    )
  
  p2 &lt;- lift_data %&gt;% 
    mutate(groupe = as.factor(groupe)) %&gt;%
    select(groupe, labs, exposure) %&gt;%
    ggplot(aes(x= groupe, y = exposure)) +
    geom_col() +
    cowplot::theme_half_open() +
    cowplot::background_grid() +
    colorblindr::scale_color_OkabeIto( )# + 
  # scale_y_continuous(
  #   expand = c(0,0),
  #   breaks  = scales::breaks_pretty(3)
  # )
  
  return(p1 / p2 +   plot_layout(heights = c(3, 1)))
}</code></pre>
<p>not too bad!</p>
<pre class="r"><code>get_lift_chart(
  data = test_w_preds,
  sort_by= pred_claim_xgboost, 
  pred = pred_claim_xgboost, 
  obs = Claim_Amount, 
  expo  = exposure )</code></pre>
<p><img src="/post/2020-03-27-tweedue-and-recipes-part-2-kaggle-data_files/figure-html/unnamed-chunk-10-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
