---
title: Tweedie vs Poisson * Gamma
author: Simon
date: '2020-03-23'
slug: tweedie-vs-poisson-gamma
categories:
  - R
  - insurnace
tags: []
keywords:
  - tech
---



```{r setup, include =F, echo =F}
#
# TODO : valider ceci : Chunk options must be written in one line; no line breaks are allowed inside chunk options;
# https://yihui.name/knitr/options/
knitr::opts_chunk$set(echo = TRUE, 
                      collapse = FALSE,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE,
                      fig.align= "center",
                      fig.width = 10,
                      highlight = TRUE,
                      cache = FALSE,
                      cache.lazy = FALSE) # fixes long vecto rnot supported quand on cache des gros éléments https://stackoverflow.com/questions/39417003/long-vectors-not-supported-yet-error-in-rmd-but-not-in-r-script
```

I'm building my first tweedie model, and I'm finally trying the {recipes} package.

We will try to predict the loss cost of car insurance policy.  

We will compare the performance of the tweedie model vs multiplying two separates models: a frequency (Poisson) and a severity (Gamma) model using  "lift charts" and "double lift charts" to evaluate the model performance.  [More details about these charts here](https://www.casact.org/education/rpm/2015/handouts/Paper_4034_handout_2419_0.pdf).  Lift charts are called "simple quantile plots" when you follow that link.  


#  Data  

The dataCar data from the `insuranceData` package.  It contains 67 856 one-year vehicle insurance policies taken out in 2004 or 2005.   

The `exposure` variable  represents the "number of year of exposure" and is used as the offset variable.  It is bounded between 0 and 1.   

Finally, the independent variables are as follow:  

* `veh_value`, the vehicle value in tens of thousand of dollars,  
* `veh_body`, y vehicle body, coded as BUS CONVT COUPE HBACK HDTOP MCARA MIBUS PANVN RDSTR SEDAN STNWG TRUCK UTE,  
* `veh_age`, 1 (youngest), 2, 3, 4,   
* `gender`, a factor with levels F M,   
* `area` a factor with levels A B C D E F,   
* `agecat` 1 (youngest), 2, 3, 4, 5, 6  


The dollar amount of the claims is  `claimcst0`.  We will divide it by the exposure to obtain `annual_loss` which is the pure annual premium.

For the frequency (Poisosn) model, we will model clm (0 or 1) because the cost variables (claimcst0) represents the total cost,  if you have multiple claims (numclaims>1).



```{r load_stuff}
# https://www.cybaea.net/Journal/2012/03/13/R-code-for-Chapter-2-of-Non_Life-Insurance-Pricing-with-GLM/
library(insuranceData) # pour les données dataCar
library(tidyverse)  # pour la manipulation de données
library(statmod) #pour glm(family = tweedie)
library(modelr) # pour add_predictions()
library(broom) # pour afficher les coefficients
library(tidymodels)
library(xgboost)
library(tictoc)
library(dviz.supp) # devtools::install_github("clauswilke/dviz.supp")
library(colorblindr)  # devtools::install_github("clauswilke/colorblindr")
library(patchwork)
library(rsample)
library(yardstick)
library(recipes)


data(dataCar)

# claimcst0claim amount (0 if no claim)
# clm = 0 or 1. numclaims = 0 , 1 ,2 ,3 or 4.. on va essayer clm (0-1).. car j'ai un claimcst0 représente
mydb <- dataCar %>% select(clm, claimcst0, exposure, veh_value, veh_body,
                           veh_age, gender, area, agecat) %>% 
  mutate(annual_loss = claimcst0 / exposure)

split <- rsample::initial_split(mydb, prop = 0.8)
train <- rsample::training(split)
test <- rsample::testing(split)


```
# Tweedie model


```{r}
tweedie_fit <- 
  glm(annual_loss ~ veh_value + veh_body + veh_age + gender + area + agecat,
      family=tweedie(var.power=1.1, link.power=0),
      weights = exposure,
      data = train)


#broom::tidy(tweedie_fit)
summary(tweedie_fit)

```

# Poisson model
```{r}

poisson_fit <-
  glm(clm ~ veh_value + veh_body + veh_age + gender + area + agecat,
      family = poisson(link = "log"),
      offset = log(exposure),
      data = train)

#broom::tidy(poisson_fit)
summary(poisson_fit)
```

# Gamma model

```{r}
gamma_fit <-
  glm(claimcst0 ~ veh_value + veh_body + veh_age + gender + area + agecat,
      data = sinistres %>% filter( claimcst0 > 0),
      family = Gamma("log"))

#broom::tidy(gamma_fit) 
summary(gamma_fit)
```

# XGBoost  



```{r}
feature_vars <- c("veh_value" ,  "veh_body" ,  "veh_age" , "gender" , "area" , "agecat")
offset_var <- "exposure"
```

## XGBoost tweedie model

pdf le fun avec weight et tou
https://pdfs.semanticscholar.org/c93b/36764413698a1f39f27969c5815363497370.pdf

```{r}
label_var <- "annual_loss" 

# one hot encoding of categorical (factor) data
myformula <- paste0( "~", paste0( feature_vars, collapse = " + ") ) %>% 
  as.formula()

dummyFier <- caret::dummyVars(myformula, data=train, fullRank = TRUE)
dummyVars.df <- predict(dummyFier,newdata = train)
train_dummy <- cbind(train %>% select(one_of(c(label_var, offset_var))), 
                     dummyVars.df)
rm(myformula, dummyFier, dummyVars.df)

# get  list the column names of the db with the dummy variables
feature_vars_dummy <-  train_dummy  %>% 
  select(-one_of(c(label_var, offset_var))) %>% 
  colnames()

# create xgb.matrix for xgboost consumption
train_xgbmatrix <- xgb.DMatrix(
  data = train_dummy %>% select(feature_vars_dummy) %>% as.matrix, 
  label = train_dummy %>% pull(label_var),
  missing = "NAN", 
  weight = train_dummy %>% pull(exposure)
)

simon_params <-list(
  booster = "gbtree",
  eta = 0.01,
  max_depth = 4,
  min_child_weight = 3,
  gamma = 0,
  subsample = 0.8,
  colsample_bytree = 0.8,
  objective = 'reg:tweedie',
  eval_metric = "tweedie-nloglik@1.1",
  tweedie_variance_power = 1.1)


set.seed(1234)

cv_tweedie <- xgb.cv(
  params = simon_params,
  data = train_xgbmatrix,
  nround = 1000,
  nfold=  5,
  #monotone_constraints = myConstraint$sens,
  prediction = TRUE,
  showsd = TRUE,
  early_stopping_rounds = 50,
  verbose = 0)

cv_tweedie$best_iteration


# xgb.cv doesnt ouput any model -- we need a model to predict test dataset
full_tweedie_model <- xgboost::xgb.train(
  data = train_xgbmatrix,
  params = simon_params,
  nrounds = cv_tweedie$best_iteration,
  nthread = parallel::detectCores() - 1
)
```

## XGBoost Poisson model



```{r}

label_var <- "clm" 

# one hot encoding of categorical (factor) data
myformula <- paste0( "~", paste0( feature_vars, collapse = " + ") ) %>% 
  as.formula()

dummyFier <- caret::dummyVars(myformula, data=train, fullRank = TRUE)
dummyVars.df <- predict(dummyFier,newdata = train)
train_dummy <- cbind(train %>% select(one_of(c(label_var, offset_var))), 
                     dummyVars.df)
rm(myformula, dummyFier, dummyVars.df)

# get  list the column names of the db with the dummy variables
feature_vars_dummy <-  train_dummy  %>% 
  select(-one_of(c(label_var, offset_var))) %>% 
  colnames()

# create xgb.matrix for xgboost consumption
train_xgbmatrix <- xgb.DMatrix(
  data = train_dummy %>% select(feature_vars_dummy) %>% as.matrix, 
  label = train_dummy %>% pull(label_var),
  missing = "NAN")

#base_margin: apply exposure offset 
setinfo(train_xgbmatrix,"base_margin", 
        train %>% pull(offset_var) %>% log() )

# a fake constraint, just to show how it is done.  
#Here we force "the older the car, the less likely are claims"
myConstraint   <- data_frame(Variable = feature_vars_dummy) %>%
  mutate(sens = ifelse(Variable == "veh_age", -1, 0))

# random folds for xgb.cv
cv_folds = rBayesianOptimization::KFold(train_dummy$clm, 
                                        nfolds= 5,
                                        stratified = TRUE,
                                        seed= 0)

simon_params <-list(
  booster = "gbtree",
  eta = 0.1,
  max_depth = 6,
  min_child_weight = 3,
  gamma = 0,
  subsample = 0.8,
  colsample_bytree = 0.8,
  objective = 'count:poisson', 
  eval_metric = "poisson-nloglik")


set.seed(1234)

cv_poisson <- xgb.cv(
  params = simon_params,
  data = train_xgbmatrix,
  nround = 200,
  folds=  cv_folds,
  prediction = TRUE,
  showsd = TRUE,
  early_stopping_rounds = 50,
  verbose = 0)

# xgb.cv doesnt ouput any model -- we need a model to predict test dataset
full_poisson_model <- xgboost::xgb.train(
  data = train_xgbmatrix,
  params = simon_params,
  nrounds = cv_poisson$best_iteration,
  nthread = parallel::detectCores() - 1
)



```


## XGBoost Gamma model

```{r}

label_var <- "claimcst0" 

# one hot encoding of categorical (factor) data
myformula <- paste0( "~", paste0( feature_vars, collapse = " + ") ) %>% 
  as.formula()

dummyFier <- caret::dummyVars(myformula, data=sinistres , fullRank = TRUE)
dummyVars.df <- predict(dummyFier,newdata = sinistres )
train_dummy <- cbind(sinistres  %>% select(one_of(c(label_var))),  #, offset_var
                     dummyVars.df)
rm(myformula, dummyFier, dummyVars.df)


# get  list the column names of the db with the dummy variables
feature_vars_dummy <-  train_dummy  %>% 
  select(-one_of(c(label_var))) %>%  # , offset_var
  colnames()

# create xgb.matrix for xgboost consumption
train_xgbmatrix <- xgb.DMatrix(
  data = train_dummy %>% select(feature_vars_dummy) %>% as.matrix, 
  label = train_dummy %>% pull(label_var),
  missing = "NAN")

simon_params <-list(
  booster = "gbtree",
  eta = 0.01,
  max_depth = 4,
  min_child_weight = 3,
  gamma = 0,
  subsample = 0.8,
  colsample_bytree = 0.8,
  objective = 'reg:gamma')


set.seed(1234)

cv_gamma <- xgb.cv(
  params = simon_params,
  data = train_xgbmatrix,
  nround = 1000,
  #folds=  cv_folds,
  nfold =5 ,
  #monotone_constraints = myConstraint$sens,
  prediction = TRUE,
  showsd = TRUE,
  early_stopping_rounds = 50,
  verbose = 0)

# xgb.cv doesnt ouput any model -- we need a gamma model to predict values on everyone that didnt get a claim.    we will run a new gamma model  on everyone who had a claim using the best number of iteration
full_gamma_model <- xgboost::xgb.train(
  data = train_xgbmatrix,
  params = simon_params,
  nrounds = cv_gamma$best_iteration,
  nthread = parallel::detectCores() - 1
)
# 
# myformula <- paste0( "~", paste0( feature_vars, collapse = " + ") ) %>% 
#   as.formula()
# dummyFier <- caret::dummyVars(myformula, data = pas_sinistres , fullRank = TRUE)
# dummyVars.df <- predict(dummyFier,newdata = pas_sinistres )
# pas_sinistres_dummy <- cbind(pas_sinistres  %>% select(one_of(c(label_var))),  #, offset_var
#                              dummyVars.df)
# rm(myformula, dummyFier, dummyVars.df)
# 
# pas_sinistres_xgbmatrix <- xgb.DMatrix(
#   data = pas_sinistres_dummy %>% select(feature_vars_dummy) %>% as.matrix, 
#   label = pas_sinistres_dummy %>% pull(label_var),
#   missing = "NAN")
# 
# 
# 
# # mettre de l'ordre dans la prédiction gamma xgboost
# 
# sinistres$pred_xgboost_gamma <- cv_gamma$pred
# pas_sinistres$pred_xgboost_gamma <- predict(full_gamma_model, pas_sinistres_xgbmatrix)
# 
# pred_xgboost_gamma <- bind_rows(
#   sinistres %>% select(row_num, pred_xgboost_gamma),
#   pas_sinistres %>% select(row_num, pred_xgboost_gamma)
# ) %>%
#   arrange(row_num) %>%
#   pull(pred_xgboost_gamma)


```


# Functions   lift   & double lift 
```{r}


# double lift charts

#' @title disloc()
#'
#' @description Cette fonction crée un tableau et une double lift chart
#' @param data data.frame  source
#' @param pred1 prediction of first model
#' @param pred1 prediction of second model
#' @param expo exposure var
#' @param obs observed result
#' @param nb nombre de quantils créés
#' @param obs_lab Label pour la valeur observée dans le graphique
#' @param pred1_lab Label pour la première prédiction dans le graphique
#' @param pred2_lab Label pour la deuxième prédiction dans le graphique
#' @param x_label Label pour la valeur réalisée dans le graphique
#' @param y_label Label pour la valeur réalisée dans le graphique
#' @param y_format Fonction utilisée pour formater l'axe des y dans le graphique (par exemple percent_format() ou dollar_format() du package scales)
#' @export


disloc <- function(data, pred1, pred2, expo, obs, nb = 10,
                   obs_lab = "",
                   pred1_lab = "", pred2_lab = "",
                   x_label = "",
                   y_label= "sinistralité",
                   y_format = scales::number_format(accuracy = 1,  big.mark = " ", decimal.mark = ",")
) {
  # obligé de mettre les variables dans un enquo pour pouvoir les utiliser dans dplyr
  
  pred1_var <- enquo(pred1)
  pred2_var <- enquo(pred2)
  expo_var <- enquo(expo)
  obs_var <- enquo(obs)
  
  
  pred1_name <- quo_name(pred1_var)
  pred2_name <- quo_name(pred2_var)
  obs_name <- quo_name(obs_var)
  
  
  if (pred1_lab =="") {pred1_lab <- pred1_name}
  if (pred2_lab =="") {pred2_lab <- pred2_name}
  if (obs_lab =="") {obs_lab <- obs_name}
  
  if (x_label == ""){ x_label <- paste0("ratio entre les prédictions ", pred1_lab, " / ", pred2_lab)}
  
  # création de la comparaison entre les deux pred
  dd <- data %>%
    mutate(ratio = !!pred1_var / !!pred2_var) %>%
    filter(!!expo_var > 0) %>%
    drop_na()
  
  # constitution des buckets de poids égaux
  dd <- dd %>% add_equal_weight_group(
    sort_by = ratio,
    expo = !!expo_var, 
    group_variable_name = "groupe",
    nb = nb
  )
  
  # comparaison sur ces buckets
  dd <- full_join(
    dd %>% group_by(groupe) %>%
      summarise(
        ratio_moyen = mean(ratio),
        ratio_min = min(ratio),
        ratio_max = max(ratio)
      ),
    dd %>% group_by(groupe) %>%
      summarise_at(
        funs(sum(.) / sum(!!expo_var)),
        .vars = vars(!!obs_var, !!pred1_var, !!pred2_var)
      ) %>%
      ungroup,
    by = "groupe"
  )
  
  # création des labels
  dd <- dd %>%
    mutate(labs = paste0("[", round(ratio_min, 2), ", ", round(ratio_max, 2), "]"))
  
  # graphe
  plotdata <-
    dd %>%
    gather(key, variable, !!obs_var, !!pred1_var, !!pred2_var) %>%
    ## Pas optimal mais je ne trouve pas mieux...
    mutate(key = case_when(
      key == obs_name ~ obs_lab,
      key == pred1_name ~ pred1_lab,
      key == pred2_name ~ pred2_lab
    )) %>%
    mutate(key = factor(key, levels = c(obs_lab, pred1_lab, pred2_lab), ordered = TRUE))
  
  pl <- plotdata %>%
    ggplot(aes(ratio_moyen, variable, color = key, linetype = key)) +
    cowplot::theme_cowplot() +
    cowplot::background_grid()+
    colorblindr::scale_color_OkabeIto( ) + 
    
    scale_x_continuous(breaks = scales::pretty_breaks())+
    geom_line() +
    geom_point() +
    scale_x_continuous(breaks = dd$ratio_moyen, labels = dd$labs) +
    scale_y_continuous(breaks = scales::pretty_breaks() )+  
    labs(
      x = x_label,
      y = y_label
    )+
    theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)) #+
  
  
  # écart au réalisé, pondéré
  ecart <- dd %>%
    mutate(poids = abs(1 - ratio_moyen)) %>%
    summarise_at(
      vars(!!pred1_var, !!pred2_var),
      funs(weighted.mean((. - !!obs_var)^2, w = poids) %>% sqrt())
    ) %>% summarise(ratio_distance = !!pred2_var / !!pred1_var) %>%
    as.numeric()
  
  list(
    graphe = pl,
    ecart = ecart,
    tableau = dd
  )
}



#' @title add_equal_weight_group()
#'
#' @description Cette fonction crée des groupe (quantiles) avec le nombre nombre total d'exposition.
#' @param table data.frame  source
#' @param sort_by Variable utilisée pour trier les observations.
#' @param expo Exposition (utilisée pour créer des quantiles de la même taille.  Si NULL, l'exposition est égale pour toutes les observations) (Défault = NULL).
#' @param nb Nombre de quantiles crées (défaut = 10)
#' @param group_variable_name Nom de la variable de groupes créée
#' @export


add_equal_weight_group <- function(table, sort_by, expo = NULL, group_variable_name = "groupe", nb = 10) {
  sort_by_var <- enquo(sort_by)
  groupe_variable_name_var <- enquo(group_variable_name)
  
  if (!(missing(expo))){ # https://stackoverflow.com/questions/48504942/testing-a-function-that-uses-enquo-for-a-null-parameter
    
    expo_var <- enquo(expo)
    
    total <- table %>% pull(!!expo_var) %>% sum
    br <- seq(0, total, length.out = nb + 1) %>% head(-1) %>% c(Inf) %>% unique
    table %>%
      arrange(!!sort_by_var) %>%
      mutate(cumExpo = cumsum(!!expo_var)) %>%
      mutate(!!group_variable_name := cut(cumExpo, breaks = br, ordered_result = TRUE, include.lowest = TRUE) %>% as.numeric) %>%
      select(-cumExpo)
  } else {
    total <- nrow(table)
    br <- seq(0, total, length.out = nb + 1) %>% head(-1) %>% c(Inf) %>% unique
    table %>%
      arrange(!!sort_by_var) %>%
      mutate(cumExpo = row_number()) %>%
      mutate(!!group_variable_name := cut(cumExpo, breaks = br, ordered_result = TRUE, include.lowest = TRUE) %>% as.numeric) %>%
      select(-cumExpo)
  }
}

get_lift_chart_data <- function(
  data, 
  sort_by,
  pred, 
  expo, 
  obs, 
  nb = 10) {
  
  pred_var <- enquo(pred)
  sort_by_var <- enquo(sort_by)
  expo_var <- enquo(expo)
  obs_var <- enquo(obs)
  
  
  pred_name <- quo_name(pred_var)
  sort_by_name <- quo_name(sort_by_var)
  obs_name <- quo_name(obs_var)
  
  # constitution des buckets de poids égaux
  dd <- data %>% add_equal_weight_group(
    sort_by = !!sort_by_var,
    expo = !!expo_var, 
    group_variable_name = "groupe",
    nb = nb
  )
  
  # comparaison sur ces buckets
  dd <- full_join(
    dd %>% 
      group_by(groupe) %>%
      summarise(
        exposure = sum(!!expo_var),
        sort_by_moyen = mean(!!sort_by_var),
        sort_by_min = min(!!sort_by_var),
        sort_by_max = max(!!sort_by_var)
      ) %>%
      ungroup(),
    dd %>% 
      group_by(groupe) %>%
      summarise_at(
        funs(sum(.) / sum(!!expo_var)),
        .vars = vars(!!obs_var, !!pred_var)
      ) %>%
      ungroup,
    by = "groupe"
  )
  
  # création des labels
  dd <- dd %>%
    mutate(labs = paste0("[", round(sort_by_min, 2), ", ", round(sort_by_max, 2), "]"))
  
}



get_lift_chart <- function(data, 
                           sort_by,
                           pred, 
                           expo, 
                           obs, 
                           nb){
  
  pred_var <- enquo(pred)
  sort_by_var <- enquo(sort_by)
  expo_var <- enquo(expo)
  obs_var <- enquo(obs)
  
  lift_data <- get_lift_chart_data(
    data = data, 
    sort_by = !!sort_by_var,
    pred = !!pred_var, 
    expo = !!expo_var, 
    obs = !!obs_var, 
    nb = 10)
  
  p1 <- lift_data %>% 
    mutate(groupe = as.factor(groupe)) %>%
    select(groupe, labs, !!pred_var, !!obs_var) %>%
    gather(key = type, value = average, !!pred_var, !!obs_var)  %>% 
    ggplot(aes(x= groupe, y = average, color =type, group = type)) +
    geom_line() +
    geom_point()+
    cowplot::theme_half_open() +
    cowplot::background_grid() +
    colorblindr::scale_color_OkabeIto( ) + 
    scale_y_continuous(
      breaks  = scales::pretty_breaks()
    )+
    theme(
      legend.position = c(0.8, 0.1),
      axis.title.x=element_blank(),
      axis.text.x=element_blank(),
      axis.ticks.x=element_blank()
    )
  
  p2 <- lift_data %>% 
    mutate(groupe = as.factor(groupe)) %>%
    select(groupe, labs, exposure) %>%
    ggplot(aes(x= groupe, y = exposure)) +
    geom_col() +
    cowplot::theme_half_open() +
    cowplot::background_grid() +
    colorblindr::scale_color_OkabeIto( ) + 
    scale_y_continuous(
      expand = c(0,0),
      breaks  = scales::breaks_pretty(3)
    )
  
  return(p1 / p2 +   plot_layout(heights = c(3, 1)))
}
```






# Add predictions to test database.  

Quick check: the means for predictions of frequency and total cost don't appear to be way off:  
```{r}
z <- test %>%
  modelr::add_predictions(. , 
                          var = "pred_annual_tweedie",
                          model = tweedie_fit,
                          type = "response") %>%
  mutate(pred_tweedie = pred_annual_tweedie * exposure) %>%
  modelr::add_predictions(. , 
                          var = "pred_poisson",
                          model = poisson_fit,
                          type = "response") %>%
  modelr::add_predictions(. , 
                          var = "pred_gamma",
                          model = gamma_fit,
                          type = "response") %>%
  mutate(pred_freq_severite = pred_poisson * pred_gamma) 






z$pred_xgboost_poisson <- cv_poisson$pred
z$pred_xgboost_tweedie <- cv_tweedie$pred
z$annual_pred_xgboost_tweedie2 <- cv_tweedie2$pred


z <- z %>% 
  mutate(pred_xgboost_tweedie2 =  annual_pred_xgboost_tweedie2 * exposure)

z$pred_xgboost_gamma <- pred_xgboost_gamma


z <- z %>% 
  mutate(pred_xgboost_freq_severite = pred_xgboost_poisson * pred_xgboost_gamma)


z %>% select(clm, pred_poisson, pred_xgboost_poisson,  claimcst0,pred_freq_severite, pred_tweedie, pred_tweedie2, pred_xgboost_freq_severite, pred_xgboost_tweedie, pred_xgboost_tweedie2) %>% summary()

```

# Lift charts of select models  


## lift chart poisson (fréquence annuelle)  

```{r}
z %>% 
  mutate(annual_rate_pred_poisson = pred_poisson / exposure) %>%
  get_lift_chart(sort_by= annual_rate_pred_poisson, pred = pred_poisson, obs = clm, expo  = exposure )



```


## lift chart tweedie (prime annuelle)  


```{r}

z %>% 
  mutate(annual_rate_tweedie = pred_tweedie / exposure) %>%
  get_lift_chart(sort_by= annual_rate_tweedie, pred = pred_tweedie, obs = claimcst0, expo  = exposure )

```




## lift chart xgboost poisson (fréquence annuelle)  

```{r}
z %>% 
  mutate(annual_rate_xgboost_pred_poisson = pred_xgboost_poisson / exposure) %>%
  get_lift_chart(sort_by= annual_rate_xgboost_pred_poisson, pred = pred_xgboost_poisson, obs = clm, expo  = exposure )

```


## lift chart xgboost  tweedie (prime annuelle)  

pas vargeux mon tweedie xgboost!    Il a pourtant terminé d'apprendre, car le best iteration `r cv_tweedie$best_iteration ` est plus petit que le 1000 permis.
```{r}

z %>% 
  mutate(annual_rate_xgboost_tweedie = pred_xgboost_tweedie / exposure) %>%
  get_lift_chart(sort_by= annual_rate_xgboost_tweedie, pred = pred_xgboost_tweedie, obs = claimcst0, expo  = exposure )

```



## lift chart xgboost  2 tweedie (prime annuelle)  

pas vargeux mon tweedie xgboost!    Il a pourtant terminé d'apprendre, car le best iteration `r cv_tweedie$best_iteration ` est plus petit que le 1000 permis.
```{r}

z %>% 
  get_lift_chart(sort_by= annual_pred_xgboost_tweedie2, pred = pred_xgboost_tweedie2, obs = claimcst0, expo  = exposure )

```

# Compare model performance with double lift chart  



## GLM Tweedie vs Poisson * Gamma   

GLM Tweedie ( power = 1.1) appears   similar to than Poisson * Gamma .  
```{r}

disloc(data = z, 
       pred1 = pred_freq_severite, 
       pred2 = pred_tweedie, 
       expo = exposure, 
       obs = claimcst0 ,
       y_label = "coût moyen ($)"
) %>% .$graphe

```

## GLM Tweedie vs XGB Tweedie  

Why is the xgboost tweedie doing so poorly vs GLM tweedie?  
partial answer: xgboost is out of fold, glm can try to overfit.. but it really doesnt have that many variables to overfit on.

```{r}

disloc(data = z, 
       pred1 = pred_xgboost_tweedie, 
       pred2 = pred_tweedie, 
       expo = exposure, 
       obs = claimcst0 ,
       y_label = "coût moyen ($)"
) %>% .$graphe

```

## GLM Poisson*Gamma  vs XGB Poisson*Gamma  


Why is the xgboost Poisson * Gamma doing so poorly vs GLM Poisson * Gamma?  
partial answer: xgboost is out of fold, glm can try to overfit.. but it really doesnt have that many variables to overfit on.
```{r}
disloc(data = z, 
       pred1 = pred_freq_severite, 
       pred2 = pred_xgboost_freq_severite, 
       expo = exposure, 
       obs = claimcst0 ,
       y_label = "coût moyen ($)"
) %>% .$graphe

```

## GLM Tweedie vs XGB Tweedie 2  

Why is the xgboost tweedie doing so poorly vs GLM tweedie?  
partial answer: xgboost is out of fold, glm can try to overfit.. but it really doesnt have that many variables to overfit on.

```{r}

disloc(data = z, 
       pred1 = pred_tweedie, 
       pred2 = pred_xgboost_tweedie2, 
       expo = exposure, 
       obs = claimcst0 ,
       y_label = "coût moyen ($)"
) %>% .$graphe

```


## XGB Tweedie vs XGB Tweedie 2  

Why is the xgboost tweedie doing so poorly vs GLM tweedie?  
partial answer: xgboost is out of fold, glm can try to overfit.. but it really doesnt have that many variables to overfit on.

```{r}

disloc(data = z, 
       pred1 = pred_xgboost_tweedie, 
       pred2 = pred_xgboost_tweedie2, 
       expo = exposure, 
       obs = claimcst0 ,
       y_label = "coût moyen ($)"
) %>% .$graphe

```



# what a minute, on essaie recipe    
https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html
https://www.andrewaage.com/post/a-simple-solution-to-a-kaggle-competition-using-xgboost-and-recipes/


If the analysis only requires outcomes and predictors, the easiest way to create the initial recipe is to use the standard formula method..
mais nous on a des bois
```{r}

```


```{r}
rec <-  recipes::recipe(train %>% select(annual_loss,exposure, veh_value,veh_body, veh_age, gender,area,agecat)) %>% 
  recipes::update_role(everything(), new_role = "predictor") %>%
  recipes::update_role(annual_loss, new_role = "outcome") %>%
  recipes::update_role(exposure, new_role = "case weight") %>%
  recipes::step_zv(recipes::all_predictors()) %>%   # remove variable with all equal values
  recipes::step_other(recipes::all_predictors(), threshold = 0.01)  %>%       # combine categories with less than 1% of observation
  recipes::step_dummy(recipes::all_nominal())  %>% # convert to dummy for xgboost use
  check_missing(all_predictors()) ## break the bake function if any of the checked columns contains NA value

# Prepare the recipe and use juice/bake to get the data!
prepped_rec <- prep(rec)
train <- juice(prepped_rec)
test <- bake(prepped_rec, new_data = test)

```

# glm 

```{r}
tweedie_fit <- 
  glm(annual_loss ~ . - exposure,
      family=tweedie(var.power=1.1, link.power=0),
      weights = exposure,
      data = train)
```



# xgboost  


```{r}
xgtrain <- xgb.DMatrix(as.matrix(train %>% select(-annual_loss , -exposure )),  
                       label = train$annual_loss,
                       weight = train$exposure
                       )


xgtest <- xgb.DMatrix(as.matrix(test %>% select(-annual_loss , -exposure )),  
                      label = test$annual_loss,
                      weight = test$exposure
                      )



params <-list(
  booster = "gbtree",
  objective = 'reg:tweedie',
  eval_metric = "tweedie-nloglik@1.1",
  tweedie_variance_power = 1.1,
  gamma = 0,
  max_depth = 4,
  eta = 0.01,
  min_child_weight = 3,
  subsample = 0.8,
  colsample_bytree = 0.8,
  tree_method = "hist"
  )

xgcv <- xgb.cv(
  params = params,
  data = xgtrain,
  nround = 1000,
  nfold=  5,
  showsd = TRUE,
  early_stopping_rounds = 50,
  verbose = 0)

cv_tweedie$best_iteration


# xgb.cv doesnt ouput any model -- we need a model to predict test dataset
xgmodel <- xgboost::xgb.train(
  data = xgtrain,
  params = params,
  nrounds = xgcv$best_iteration,
  nthread = parallel::detectCores() - 1
)

```

```{r}
testplus <- test %>%
  add_predictions("pred_glm", model = tweedie_fit, type = "response") %>%
  mutate(claim = annual_loss * exposure,
         pred_claim = pred_glm * exposure) %>%
  mutate(pred_xgboost = predict(xgmodel, xgtest),
         pred_xgboost_claim = pred_xgboost * exposure)
  
testplus %>% select(annual_loss, pred_glm, pred_xgboost, pred_claim, claim, pred_xgboost_claim) %>% summary


testplus %>% 
  get_lift_chart(sort_by= pred_glm, pred = pred_claim, obs = claim, expo  = exposure )


testplus %>% 
  get_lift_chart(sort_by= pred_xgboost, pred = pred_xgboost_claim, obs = claim, expo  = exposure )


disloc(data = testplus, 
       pred1 = pred_claim, 
       pred2 = pred_xgboost_claim, 
       expo = exposure, 
       obs = claim ,
       y_label = "coût moyen ($)"
) %>% .$graphe
```


